{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Spark SQL in Python**"
      ],
      "metadata": {
        "id": "UrAuo9R68Dk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим таблицу SQL из dataframe (фундаментальная абстракция данных Spark). Spark DataFrame — это распределенная коллекция данных, организованных в именованные столбцы."
      ],
      "metadata": {
        "id": "6UzH0_PZBfH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(filename, header=True) # получаем dataframe, тип <class 'pyspark.sql.dataframe.DataFrame'>\n",
        "df.createOrReplaceTempView(\"schedule\") # тип <class 'pyspark.sql.dataframe.DataFrame'>\n",
        "spark.sql(\"SELECT * FROM schedule WHERE station = 'San Jose'\").show() # создаем таблицу Spark"
      ],
      "metadata": {
        "id": "L_3rmMpz8FbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# все способы получить имена столбцов\n",
        "result = spark.sql(\"SHOW COLUMNS FROM tablename\")\n",
        "result = spark.sql(\"SELECT * FROM tablename LIMIT 0\")\n",
        "result = spark.sql(\"DESCRIBE tablename\") # таблица с именами и типами\n",
        "result.show()\n",
        "print(result.columns)"
      ],
      "metadata": {
        "id": "xMsTCbldD6xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оконная функция"
      ],
      "metadata": {
        "id": "f5dMC7ExO-xL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"SELECT train_id, station, time, diff_min,\n",
        "           SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
        "           FROM schedule\"\"\"\n",
        "spark.sql(query).show()\n",
        "+--------+-------------+-----+--------+-------------+\n",
        "|train_id|      station| time|diff_min|running_total|\n",
        "+--------+-------------+-----+--------+-------------+\n",
        "|     217|       Gilroy|6:06a|     9.0|          9.0|\n",
        "|     217|   San Martin|6:15a|     6.0|         15.0|\n",
        "|     324|San Francisco|7:59a|     4.0|          4.0|\n",
        "|     324|  22nd Street|8:03a|    13.0|         17.0|"
      ],
      "metadata": {
        "id": "boqFpurbrr_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"SELECT train_id, station, time, LEAD(time, 1) OVER (ORDER BY time) AS time_next\n",
        "           FROM sched\n",
        "           WHERE train_id=324 \"\"\"\n",
        "spark.sql(query).show()\n",
        "\n",
        "SELECT # тут PARTITION BY\n",
        "train_id,\n",
        "station,\n",
        "time,\n",
        "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next\n",
        "#добавление предложения OVER определяет этот запрос как запрос оконной функции\n",
        "#Функция LEAD позволяет запрашивать более одной строки в таблице за раз без необходимости присоединения таблицы к самой себе\n",
        "FROM sched"
      ],
      "metadata": {
        "id": "iATSjJFCMMC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точечная нотация"
      ],
      "metadata": {
        "id": "xCeTrAtwPBmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\n",
        "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('MIN(time)', 'start').show()\n",
        "\n",
        "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\n",
        "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
        "result.show()\n",
        "print(result.columns[1]) #max(time)"
      ],
      "metadata": {
        "id": "ghcoyXB7t0Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max, col\n",
        "expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\n",
        "dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
        "dot_df.show()\n",
        "+--------+-----+-----+\n",
        "|train_id|start|  end|\n",
        "+--------+-----+-----+\n",
        "|     217|6:06a|6:59a|\n",
        "|     324|7:59a|9:05a|\n",
        "+--------+-----+-----+\n",
        "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\""
      ],
      "metadata": {
        "id": "QTcOhrYdukps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n",
        "#['train_id', 'station', 'time']\n",
        "df.show(5)\n",
        "df.select('train_id', 'station').show(5) #могли бы так\n",
        "\n",
        "df.select('train_id', 'station')\n",
        "df.select(df.train_id, df.station)\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col('train_id'), col('station')) # пример когда полезен...\n",
        "\n",
        "df.select('train_id', 'station').withColumnRenamed('train_id', 'train').show(5)\n",
        "df.select(col('train_id').alias('train'), 'station') #можно так"
      ],
      "metadata": {
        "id": "9a5Jz_7-PEnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SELECT train_id AS train, station FROM schedule LIMIT 5').show()\n",
        "df.select(col('train_id').alias('train'), 'station').limit(5).show()"
      ],
      "metadata": {
        "id": "SQ8Y9LiXV6lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# оконные функции тоже в точечной и sql нотации\n",
        "query = \"\"\"SELECT *,\n",
        "           ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id\n",
        "           FROM schedule\"\"\"\n",
        "spark.sql(query).show(11) # или точечная нотация...\n",
        "\n",
        "from pyspark.sql import Window,\n",
        "from pyspark.sql.functions import row_number\n",
        "df.withColumn(\"id\", row_number().over(Window.partitionBy('train_id').orderBy('time')))\n",
        "# пример\n",
        "dot_df = df.withColumn('time_next', lead('time', 1).over(Window.partitionBy('train_id')\n",
        "        .orderBy('time'))).show()"
      ],
      "metadata": {
        "id": "9rr-l0g6W04r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ROW_NUMBER in SQL : pyspark.sql.functions.row_number\n",
        "* The inside of the OVER clause : pyspark.sql.Window\n",
        "* PARTITION BY : pyspark.sql.Window.partitionBy\n",
        "* ORDER BY : pyspark.sql.Window.orderBy"
      ],
      "metadata": {
        "id": "FHZ6rOMPc5NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window = Window.partitionBy('train_id').orderBy('time')\n",
        "dfx = df.withColumn('next', lead('time', 1).over(window))"
      ],
      "metadata": {
        "id": "j4lFedc8dB1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "query = \"\"\"SELECT *,\n",
        "          (UNIX_TIMESTAMP(LEAD(time, 1) OVER(PARTITION BY train_id ORDER BY time),'H:m')\n",
        "          - UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min\n",
        "          FROM schedule\"\"\"\n",
        "sql_df = spark.sql(query)\n",
        "sql_df.show()\n",
        "\n",
        "window = Window.partitionBy('train_id').orderBy('time')\n",
        "dot_df = df.withColumn('diff_min', (unix_timestamp(lead('time', 1).over(window),'H:m')\n",
        "                     - unix_timestamp('time', 'H:m'))/60)\n",
        "    +--------+-------------+-----+--------+\n",
        "    |     217|       Gilroy|6:06a|     9.0|\n",
        "    |     217|   San Martin|6:15a|     6.0|\n",
        "    |     217|  Morgan Hill|6:21a|    15.0|"
      ],
      "metadata": {
        "id": "JiQKxJWYxiPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Использование оконной функции SQL для обработки естественного языка"
      ],
      "metadata": {
        "id": "l7l1V-TpdfoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.text('sherlock.txt')\n",
        "print(df.first()) # получить первую строку\n",
        "# Row(value='The Project Gutenberg EBook of The Adventures of Sherlock Holmes')\n",
        "print(df.count())\n",
        "# 5500\n",
        "df1.show(15, truncate=False)"
      ],
      "metadata": {
        "id": "DflVB1bRdiV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Для загрузки файла паркета. Parquet — это формат файлов Hadoop для хранения структур данных\n",
        "```\n",
        "# df1 = spark.read.load('sherlock.parquet')\n",
        "# df1.where('id > 70').show(5, truncate=False)\n",
        "```"
      ],
      "metadata": {
        "id": "m0XGzCUAelv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df1.select(lower(col('value')))\n",
        "print(df.first())\n",
        "# Row(lower(value)= 'the project gutenberg ebook of the adventures of sherlock holmes')\n",
        "df.columns\n",
        "# ['lower(value)']\n",
        "\n",
        "df = df1.select(lower(col('value')).alias('v'))\n",
        "df.columns\n",
        "# ['v']"
      ],
      "metadata": {
        "id": "bT5vVDYEep46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df1.select(regexp_replace('value', 'Mr\\.', 'Mr').alias('v'))\n",
        "# \"Mr. Holmes.\" ==> \"Mr Holmes.\"\n",
        "df = df1.select(regexp_replace('value', 'don\\'t', 'do not').alias('v'))\n",
        "# \"don't know.\" ==> \"do not know.\"\n",
        "#Чтобы точка не интерпретировалась как специальный символ во втором аргументе, мы ставим перед ней обратную косую черту\n",
        "df = df2.select(split('v', '[ ]').alias('words')) # можно просто ' '\n",
        "df.show(truncate=False) # разедлит и поместит в список"
      ],
      "metadata": {
        "id": "qgJtywLGgahM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\n",
        "df3 = df2.select(split('v', '[ %s]' % punctuation).alias('words'))\n",
        "df3.show(truncate=False)"
      ],
      "metadata": {
        "id": "SavoM6Us6YHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = df3.select(explode('words').alias('word')) # берет массив и помещает каждый элемент в свою строку\n",
        "df4.show()\n",
        "print(df4.count())\n",
        "\n",
        "nonblank_df = df.where(length('word') > 0) # удаление пустых строк\n",
        "\n",
        "df2 = df.select('word', monotonically_increasing_id().alias('id'))\n",
        "df2.show() #создаем столбец целых чисел увеличивающихся\n",
        "# выход"
      ],
      "metadata": {
        "id": "n88bMscu6zVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.withColumn('title', when(df.id < 25000, 'Preface').when(df.id < 50000, 'Chapter 1')\n",
        "                                                          .when(df.id < 75000, 'Chapter 2')\n",
        "                                                          .otherwise('Chapter 3'))\n",
        "df2 = df2.withColumn('part', when(df2.id < 25000, 0).when(df2.id < 50000, 1)\n",
        "                                                    .when(df2.id < 75000, 2)\n",
        "                                                    .otherwise(3))\n",
        "                                                    .show()\n",
        "df2 = df.repartition(4, 'part') # перераспределение данных в df\n",
        "print(df2.rdd.getNumPartitions()) # 4 - кол-во разделов"
      ],
      "metadata": {
        "id": "eMQ3ouBWCjFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "$ ls sherlock_parts # пусть есть папка с 14 файлами\n",
        "df_parts = spark.read.text('sherlock_parts') # загрузить все текстовые файлы в папке в фрейм данных\n",
        "# файлы считываются параллельно и распределяются по нескольким разделам"
      ],
      "metadata": {
        "id": "Lph9HLvWSV1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оконные функции"
      ],
      "metadata": {
        "id": "cMX4Pt-gUjGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('part', 'title').distinct().sort('part').show(truncate=False)\n",
        "query = \"\"\"SELECT id, word AS w1,\n",
        "          LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
        "          LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3\n",
        "          FROM df\"\"\"\n",
        "spark.sql(query).sort('id').show()\n",
        "+---+----------+----------+----------+\n",
        "| id|   w1     |     w2   |        w3|\n",
        "+---+----------+----------+----------+\n",
        "| 0 |       the|   project| gutenberg|\n",
        "| 1 |   project| gutenberg|     ebook|\n",
        "| 2 | gutenberg|     ebook|        of|\n",
        "\n",
        "#тоже самое но через LAG\n",
        "lag_query = \"\"\"SELECT id,\n",
        "            LAG(word,2) OVER(PARTITION BY part ORDER BY id ) AS w1,\n",
        "            LAG(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
        "            word AS w3\n",
        "            FROM df\n",
        "            ORDER BY id\"\"\"\n",
        "spark.sql(lag_query).show()\n",
        "+---+----------+----------+----------+\n",
        "| id|        w1|        w2|        w3|\n",
        "+---+----------+----------+----------+\n",
        "| 0|       null|      null|       the|\n",
        "| 1|       null|       the|   project|\n",
        "| 2|        the|   project| gutenberg|\n",
        "| 3|    project| gutenberg|     ebook|\n",
        "# если смотрим на результат для более позднего раздела, используя WHERE part=2\n",
        "lag_query = \"\"\"SELECT id,\n",
        "            LAG(word,2) OVER(PARTITION BY part ORDER BY id ) AS w1,\n",
        "            LAG(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
        "            word AS w3\n",
        "            FROM df\n",
        "            WHERE part=2\"\"\"\n",
        "spark.sql(lag_query).show()\n",
        "+----+----------+----------+----------+\n",
        "|  id|        w1|        w2|        w3|\n",
        "+----+----------+----------+----------+\n",
        "|8859|      null|      null|     part2|\n",
        "|8860|      null|     part2| adventure|\n",
        "|8861|     part2| adventure|        ii|"
      ],
      "metadata": {
        "id": "MalxslzHUmFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Какие слова встречаются вместе"
      ],
      "metadata": {
        "id": "7Jh6B-JCrZFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = \"\"\"SELECT id,\n",
        "            word AS w1,\n",
        "            LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
        "            LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3\n",
        "            FROM df\"\"\"\n",
        "\n",
        "query3agg = \"\"\"SELECT w1, w2, w3, COUNT(*) as count FROM (\n",
        "                      SELECT word AS w1,\n",
        "                      LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
        "                      LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3\n",
        "                      FROM df)\n",
        "GROUP BY w1, w2, w3\n",
        "ORDER BY count DESC\"\"\"\n",
        "spark.sql(query3agg).show() # считаем какие тройки вместе встречаются\n",
        "\n",
        "query3agg = \"\"\"SELECT w1, w2, w3, length(w1)+length(w2)+length(w3) as length\n",
        "              FROM (SELECT word AS w1,\n",
        "               LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
        "               LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3\n",
        "               FROM df\n",
        "               WHERE part <> 0 and part <> 13)\n",
        "            GROUP BY w1, w2, w3\n",
        "            ORDER BY length DESC\"\"\"\n",
        "spark.sql(query3agg).show(truncate=False)"
      ],
      "metadata": {
        "id": "50CYWWhtrc40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Опять же, обратите внимание, что Spark может выполнять этот запрос параллельно для нескольких рабочих процессов, и нам не нужно указывать, как именно это сделать. Поскольку данные секционированы, Spark может автоматически распараллелить оконную функцию SQL."
      ],
      "metadata": {
        "id": "cCy0n7eTvefU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кэширование"
      ],
      "metadata": {
        "id": "2UU5KzrMwixv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache() # кэшировать dataframe\n",
        "df.unpersist() # раскешировать\n",
        "df.is_cached # был ли вэширован df"
      ],
      "metadata": {
        "id": "WF79nkNRwkhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Уровень хранения кадра данных указывает 5 сведений о том, как он кэшируется:\n",
        "* useDisk = True\n",
        "* useMemory = True\n",
        "* useOffHeap = False\n",
        "* deserialized = True\n",
        "* replication = 1"
      ],
      "metadata": {
        "id": "XLf3AJfmWaFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.storageLevel\n",
        "#StorageLevel(True, True, False, True, 1)"
      ],
      "metadata": {
        "id": "cR9xgvM3vfQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#кэширование таблицы\n",
        "df.createOrReplaceTempView('df')\n",
        "spark.catalog.isCached(tableName='df')\n",
        "\n",
        "spark.catalog.cacheTable('df') # кэширование\n",
        "spark.catalog.isCached(tableName='df') # была ли кэширована\n",
        "spark.catalog.uncacheTable('df') # раскэшировать\n",
        "spark.catalog.clearCache() # удаление кэшированных таблиц\n",
        "spark.catalog.dropTempView('table1') # удаление временная таблица из каталога\n",
        "\n",
        "spark.catalog.listTables() #[Table(name='text',database=None,description=None,tableType='TEMPORARY'"
      ],
      "metadata": {
        "id": "pkT2pWw1YCMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ведение журнала (lagging)"
      ],
      "metadata": {
        "id": "T9y1Ea7t0b0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logging.info(\"Hello %s\", \"world\")\n",
        "logging.debug(\"Hello, take %d\", 2)\n",
        "# 2019-03-14 15:92:65,359 - INFO - Hello world\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logging.info(\"Hello %s\", \"world\")\n",
        "logging.debug(\"Hello, take %d\", 2)\n",
        "# 2018-03-14 12:00:00,000 - INFO - Hello world\n",
        "# 2018-03-14 12:00:00,001 - DEBUG - Hello, take 2"
      ],
      "metadata": {
        "id": "2oD0efIo0chw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = timer()\n",
        "t.elapsed()\n",
        "# 1. elapsed: 0.0 sec\n",
        "t.elapsed() # Do something that takes 2 seconds\n",
        "# 2. elapsed: 2.0 sec\n",
        "t.reset() # Do something else that takes time: reset\n",
        "t.elapsed()\n",
        "# 3. elapsed: 0.0 sec\n",
        "\n",
        "class timer:\n",
        "  start_time = time.time()\n",
        "  step = 0\n",
        "  def elapsed(self, reset=True):\n",
        "    self.step += 1\n",
        "    print(\"%d. elapsed: %.1f sec %s\" % (self.step, time.time() - self.start_time))\n",
        "  if reset:\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.start_time = time.time()"
      ],
      "metadata": {
        "id": "7TDAyt3r1RuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "# < create dataframe df here >\n",
        "t = timer()\n",
        "logging.info(\"No action here.\")\n",
        "t.elapsed()\n",
        "logging.debug(\"df has %d rows.\", df.count())\n",
        "t.elapsed()"
      ],
      "metadata": {
        "id": "cPV9k9Gm1xjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENABLED = False\n",
        "t = timer()\n",
        "logger.info(\"No action here.\")\n",
        "t.elapsed()\n",
        "if ENABLED:\n",
        "    logger.info(\"df has %d rows.\", df.count())\n",
        "t.elapsed()\n",
        "# 2019-03-14 12:34:56,789 - Pyspark - INFO - No action here.\n",
        "# 1. elapsed: 0.0 sec\n",
        "# 2. elapsed: 0.0 sec\n",
        "\n",
        "# 2019-03-14 12:34:56,789 - INFO - No action here.\n",
        "# 1. elapsed: 0.0 sec\n",
        "# 2019-03-14 12:34:58,789 - INFO - df has 1107014 rows.\n",
        "# 2. elapsed: 2.0 sec"
      ],
      "metadata": {
        "id": "wA6CDh-l17z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Планы запросов - предоставит подробную информацию о плане без его фактического выполнения"
      ],
      "metadata": {
        "id": "75juBoZWAdNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.load('/temp/df.parquet')\n",
        "df.registerTempTable('df')\n",
        "spark.sql('EXPLAIN SELECT * FROM df').first()\n",
        "\n",
        "#Row(plan='== Physical Plan ==\\n\n",
        "#*FileScan parquet [word#1928,id#1929L,title#1930,part#1931] - имена столбцов\n",
        "# Batched: true,\n",
        "# Format: Parquet,\n",
        "# Location: InMemoryFileIndex[file:/temp/df.parquet],\n",
        "# PartitionFilters: [],\n",
        "# PushedFilters: [],\n",
        "# ReadSchema: struct<word:string,id:bigint,title:string,part:int>') - типы столбцов"
      ],
      "metadata": {
        "id": "O34WK4RWAnyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.explain() # можно для фрейма данных (результат будет аналогичен)\n",
        "df.cache() #кэширование\n",
        "df.explain() # чтение снизу вверх"
      ],
      "metadata": {
        "id": "_vvRNF0VCc2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Классификация текстов"
      ],
      "metadata": {
        "id": "qtIkR-MKgwpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, explode\n",
        "df.where(length('sentence') == 0) # выдаст строки с пустыми строками"
      ],
      "metadata": {
        "id": "pLq5MnEFgztl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание пользовательских функций (выдает True если длина меньше 10)\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import BooleanType #указать Spark тип, который должен быть возвращен нашей новой пользовательской функцией\n",
        "\n",
        "short_udf = udf(lambda x: True if not x or len(x) < 10 else False,\n",
        "                          BooleanType()) #один аргумент x, указывающий, что она работает с одним столбцом\n",
        "df.select(short_udf('textdata').alias(\"is short\")).show(3)\n",
        "+--------+\n",
        "|is short|\n",
        "+--------+\n",
        "|   false|\n",
        "|    true|\n",
        "|   false|"
      ],
      "metadata": {
        "id": "0KG69bxZnPAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType, IntegerType, FloatType, ArrayType\n",
        "df3.select('word array', in_udf('word array').alias('without endword')).show(5, truncate=30)\n",
        "+-----------------------------+----------------------+\n",
        "|                   word array|       without endword|\n",
        "+-----------------------------+----------------------+\n",
        "|[then, how, many, are, there]|[then, how, many, are]|\n",
        "|                  [how, many]|                 [how]|\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "in_udf = udf(lambda x:\n",
        "    x[0:len(x)-1] if x and len(x) > 1\n",
        "    else [],\n",
        "    ArrayType(StringType()))"
      ],
      "metadata": {
        "id": "pHk4l2wBtC6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# разреженные vs плотные\n",
        "hasattr(x, \"toArray\") #определить, что объект является разреженным вектором\n",
        "x.numNonzeros() #определить, что вектор пуст"
      ],
      "metadata": {
        "id": "44d3ZfAMy1eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf # конспект\n",
        "from pyspark.sql.types import IntegerType\n",
        "bad_udf = udf(lambda x: x.indices[0]\n",
        "              if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
        "              else 0, IntegerType()) # будет ошибка\n",
        "try:\n",
        "    df.select(bad_udf('outvec').alias('label')).first()\n",
        "except Exception as e:\n",
        "    print(e.__class__)\n",
        "    print(e.errmsg)\n",
        "# <class 'py4j.protocol.Py4JJavaError'>\n",
        "# An error occurred while calling o90.collectToPython.\n",
        "\n",
        "first_udf = udf(lambda x: int(x.indices[0])\n",
        "                if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
        "                else 0,\n",
        "                IntegerType()) # всё ок\n",
        "+-------+--------------------+-----+--------------------+-------------------+\n",
        "|endword|                 doc|count|            features|             outvec|\n",
        "+-------+--------------------+-----+--------------------+-------------------+\n",
        "|     it|[please, do, not,...| 1149|(12847,[15,47,502...|  (12847,[7],[1.0])|\n",
        "\n",
        "df.withColumn('label', k_udf('outvec')).drop('outvec').show(3)\n",
        "+-------+--------------------+-----+--------------------+-----+\n",
        "|endword|                 doc|count|            features|label|\n",
        "+-------+--------------------+-----+--------------------+-----+\n",
        "|     it|[please, do, not,...| 1149|(12847,[15,47,502...|    7|"
      ],
      "metadata": {
        "id": "PUtbDTS0604B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer(inputCol='words', outputCol=\"features\")\n",
        "model = cv.fit(df)\n",
        "result = model.transform(df)\n",
        "print(result)\n",
        "\n",
        "DataFrame[words: array<string>, features: vector]\n",
        "# Dense string array on left, dense integer vector on right\n",
        "+-------------------------+--------------------------------------+\n",
        "|                   words |                             features |\n",
        "+-------------------------+--------------------------------------+\n",
        "|          [Hello, world] |                 (10,[7,9],[1.0,1.0]) |\n",
        "|        [How, are, you?] |           (10,[1,3,4],[1.0,1.0,1.0]) |\n",
        "|[I, am, fine, thank, you]|(10,[0,2,5,6,8],[1.0,1.0,1.0,1.0,1.0])|\n",
        "# первая строка -  длина разреженного вектора 10, 7 и 9 место в словаре и встречаются один раз"
      ],
      "metadata": {
        "id": "ISS1deFLDM7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_true = df.where(\"endword in ('she','he','hers','his','her', 'him')\").withColumn('label', lit(1))\n",
        "df_false = df.where(\"endword not in ('she','he','hers','his','her','him')\").withColumn('label', lit(0))\n",
        "df_examples = df_true.union(df_false)\n",
        "df_train, df_eval = df_examples.randomSplit((0.60, 0.40), 42)\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "logistic = LogisticRegression(maxIter=50, regParam=0.6, elasticNetParam=0.3)\n",
        "model = logistic.fit(df_train)\n",
        "print(\"Training iterations: \", model.summary.totalIterations)"
      ],
      "metadata": {
        "id": "cCITjtlvOxgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = df_trained.transform(df_test) # добавляет столбец прогнозирования и вероятности\n",
        "x = predicted.first\n",
        "print(\"Right!\" if x.label == int(x.prediction) else \"Wrong\") # вырный прогноз или нет"
      ],
      "metadata": {
        "id": "4amE6ooTvHPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_stats = model.evaluate(df_eval)\n",
        "type(model_stats)\n",
        "# pyspark.ml.classification.BinaryLogisticRegressionSummary)\n",
        "print(\"\\nPerformance: %.2f\" % model_stats.areaUnderROC)"
      ],
      "metadata": {
        "id": "4Z3TYg8SyM5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Введение в PySpark**"
      ],
      "metadata": {
        "id": "0duPNfNsyDnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# через SparkContext как sc - подключение к кластеру\n",
        "from pyspark.sql import SparkSession\n",
        "my_spark = SparkSession.builder.getOrCreate()# интерфейс для подключения (интерфейс dataframe)\n",
        "\n",
        "print(spark.catalog.listTables()) # catalog атрибут данных внутри кластера\n",
        "\n",
        "flights10 = spark.sql(\"FROM flights SELECT * LIMIT 10\")\n",
        "# нет таблицы в качестве аргумента т.к нет ее локально\n",
        "\n",
        "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
        "flight_counts = spark.sql(query)\n",
        "pd_counts = flight_counts.toPandas() # локально через pandas\n",
        "print(pd_counts.head()) #перевод из Spark DataFrame в pandas DataFrame\n",
        "\n",
        "# из pandas в Spark но данные хранятся локально а не в каталоге SparkSession\n",
        "# методы Spark DataFrame можно а вот доступ к данным в других контекстах нет\n",
        "# если .sql() то выдаст ошибку (надо временную таблицу)\n",
        "# регистрация как таблицу в каталоге, но доступ к ней возможен только из конкретной SparkSession\n",
        "spark_temp = spark.createDataFrame(pd.DataFrame(np.random.random(10)))\n",
        "\n",
        "spark_temp.createOrReplaceTempView('temp')\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "airports = spark.read.csv(\"/usr/local/share/datasets/airports.csv\", header=True)\n",
        "airports.show()"
      ],
      "metadata": {
        "id": "IZ-N3d7ByKUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Манипулирование данными"
      ],
      "metadata": {
        "id": "87eaGnh3C7eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights = spark.table(\"flights\") # чтобы создать фрейм данных, содержащий значения таблицы flights в .catalog\n",
        "flights.show()\n",
        "flights = flights.withColumn(\"duration_hrs\", flights.air_time/60)\n",
        "\n",
        "flights.filter(\"air_time > 120\").show()\n",
        "flights.filter(flights.air_time > 120).show()\n",
        "\n",
        "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
        "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
        "filterA = flights.origin == \"SEA\"\n",
        "filterB = flights.dest == \"PDX\"\n",
        "selected2 = temp.filter(filterA).filter(filterB)\n",
        "\n",
        "flights.select((flights.air_time/60).alias(\"duration_hrs\"))\n",
        "flights.selectExpr(\"air_time/60 as duration_hrs\") # или так аналогичный результат\n",
        "\n",
        "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
        "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
        "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")"
      ],
      "metadata": {
        "id": "N7RKIJr7C93d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min(), .max() и .count() методы GroupedData\n",
        "df.groupBy().min(\"col\").show() # создается объект GroupedData и в конце dataframe"
      ],
      "metadata": {
        "id": "H7gi_s5TNm8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n",
        "flights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()\n",
        "\n",
        "flights.filter(flights.carrier == \"DL\").filter(flights.origin == \"SEA\").groupBy().avg(\"air_time\").show()\n",
        "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()\n",
        "\n",
        "by_plane = flights.groupBy(\"tailnum\")\n",
        "by_plane.count().show()\n",
        "by_origin = flights.groupBy(\"origin\")\n",
        "by_origin.avg(\"air_time\").show()\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "by_month_dest = flights.groupBy('month', 'dest')\n",
        "by_month_dest.avg('dep_delay').show()\n",
        "# Этот метод позволяет передать агрегатное выражение столбца, в котором используется любая\n",
        "# агрегатная функция из подмодуля pyspark.sql.functions\n",
        "by_month_dest.agg(F.stddev('dep_delay')).show()\n",
        "\n",
        "\n",
        "# объединение\n",
        "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
        "flights_with_airports = flights.join(airports, on = 'dest', how = \"leftouter\")"
      ],
      "metadata": {
        "id": "_YrrLFOIcbgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast('integer'))# integer или double\n",
        "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)\n",
        "#очистка от пропущенных\n",
        "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")"
      ],
      "metadata": {
        "id": "yKxH8kmxZPHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\") # число строковой категории\n",
        "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\") # one-hot (конспект)\n",
        "vec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol=\"features\")\n",
        "# выше сибираем все признаки в один вектор и метка\n",
        "from pyspark.ml import Pipeline\n",
        "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])\n",
        "\n",
        "import pyspark.ml.evaluation as evals\n",
        "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\") # как печатать"
      ],
      "metadata": {
        "id": "yLZ4wCLxee0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.ml.tuning as tune\n",
        "grid = tune.ParamGridBuilder()\n",
        "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
        "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
        "grid = grid.build() # выводит сетку\n",
        "\n",
        "cv = tune.CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
        "models = cv.fit(training)\n",
        "best_lr = models.bestModel\n",
        "test_results = best_lr.transform(test)\n",
        "print(evaluator.evaluate(test_results))"
      ],
      "metadata": {
        "id": "sF9vLuYBkTC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Big Data Fundamentals with PySpark**"
      ],
      "metadata": {
        "id": "wTv4GlvAPeN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SparkContext как sc это типо ключа от двери\n",
        "sc.version # версия Spark\n",
        "sc.pythonVer # версия Python\n",
        "sc.master #local[*] это URL-адрес кластера\n",
        "\n",
        "# необработанные данные в PySpark с помощью SparkContext\n",
        "rdd = sc.parallelize([1,2,3,4,5]) # PythonRDD[1] at RDD at PythonRDD.scala:53\n",
        "rdd2 = sc.textFile(\"test.txt\")\n",
        "#/usr/local/share/datasets/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ],
      "metadata": {
        "id": "fpPrrIoXPgjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функциональные объекты и скрытые функции\n",
        "items = [1, 2, 3, 4]\n",
        "list(map(lambda x: x + 2 , items)) # [3, 4, 5, 6]\n",
        "list(filter(lambda x: (x%2 != 0), items)) # [1, 3]"
      ],
      "metadata": {
        "id": "kq5MjzQhY8bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание RDD создание, подгрузка, из других RDD\n",
        "numRDD = sc.parallelize([1,2,3,4]) # <class 'pyspark.rdd.RDD'>\n",
        "helloRDD = sc.parallelize(\"Hello world\")\n",
        "type(helloRDD)\n",
        "\n",
        "fileRDD = sc.textFile(\"README.md\")\n",
        "type(fileRDD)\n",
        "\n",
        "numRDD = sc.parallelize(range(10), minPartitions = 6) # разделы\n",
        "fileRDD = sc.textFile(\"README.md\", minPartitions = 6)\n",
        "# узнать через  getNumPartitions()\n",
        "\n",
        "RDD = sc.parallelize([1,2,3,4])\n",
        "RDD_map = RDD.map(lambda x: x * x) # [1, 4, 9, 16]\n",
        "RDD_filter = RDD.filter(lambda x: x > 2) # [3, 4]\n",
        "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
        "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \")) # [\"hello\", \"world\", \"how\", \"are\", \"you\"]\n",
        "\n",
        "inputRDD = sc.textFile(\"logs.txt\")\n",
        "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split()) # [...,...,...] где есть error\n",
        "warningsRDD = inputRDD.filter(lambda x: \"warnings\" in x.split())\n",
        "combinedRDD = errorRDD.union(warningsRDD)\n",
        "\n",
        "RDD_map.collect() # [1, 4, 9, 16]\n",
        "RDD_map.take(2) # [1, 4]\n",
        "RDD_map.first() # [1]\n",
        "RDD_flatmap.count() # 5"
      ],
      "metadata": {
        "id": "tU5itl8biteK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#практика\n",
        "cubedRDD = numbRDD.map(lambda x: x**3) #PythonRDD[1] at collect at <ipython-input-1-f3ba073d143f>:5\n",
        "numbers_all = cubedRDD.collect() # [...]\n",
        "\n",
        "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
        "for line in fileRDD_filter.take(4): # выдаст первые 4 строки\n",
        "  print(line)"
      ],
      "metadata": {
        "id": "_3vhFeAVv-ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тип данных парный RDD - ключ/значение"
      ],
      "metadata": {
        "id": "aWMd0LnwyXyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# расширенные преобразования\n",
        "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
        "pairRDD_tuple = sc.parallelize(my_tuple) # создание либо либо\n",
        "\n",
        "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
        "regularRDD = sc.parallelize(my_list)\n",
        "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
        "# операции для pair RDD\n",
        "regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)])\n",
        "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y) # операции с одинаковыми ключами\n",
        "pairRDD_reducebykey.collect()\n",
        "# [('Neymar', 22), ('Ronaldo', 34), ('Messi', 47)]\n",
        "\n",
        "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
        "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect() # сортировка по ключу\n",
        "# [(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]\n",
        "\n",
        "airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US, \"SFO\")]\n",
        "regularRDD = sc.parallelize(airports)\n",
        "pairRDD_group = regularRDD.groupByKey().collect()\n",
        "for cont, air in pairRDD_group:\n",
        "    print(cont, list(air)) # группировка по ключу\n",
        "# FR ['CDG']\n",
        "# US ['JFK', 'SFO']\n",
        "# UK ['LHR']\n",
        "\n",
        "RDD1 = sc.parallelize([(\"Messi\", 34), (\"Ronaldo\", 32), (\"Neymar\", 24)])\n",
        "RDD2 = sc.parallelize([(\"Ronaldo\", 80), (\"Neymar\", 120),(\"Messi\", 100)])\n",
        "RDD1.join(RDD2).collect() # объединение по ключу\n",
        "# [('Neymar', (24, 120)), ('Ronaldo', (32, 80)), ('Messi', (34, 100))]"
      ],
      "metadata": {
        "id": "_M8qybfWyl5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# расширенные действия\n",
        "x = [1,3,4,6]\n",
        "RDD = sc.parallelize(x)\n",
        "RDD.reduce(lambda x, y : x + y) # 14\n",
        "\n",
        "# сохранение RDD в каталоге\n",
        "RDD.saveAsTextFile(\"tempFile\")\n",
        "RDD.coalesce(1).saveAsTextFile(\"tempFile\") # сохранение как один файл\n",
        "\n",
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "for kee, val in rdd.countByKey().items(): # подсчет ключей\n",
        "    print(kee, val)\n",
        "# ('a', 2)\n",
        "# ('b', 1)\n",
        "sc.parallelize([(1, 2), (3, 4)]).collectAsMap() # просто в список\n",
        "# {1: 2, 3: 4}"
      ],
      "metadata": {
        "id": "Z74u-xyY44bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark SQL — это библиотека Spark для структурированных данных. В отличие от RDD PySpark SQL предоставляет больше информации о структуре данных и выполняемых вычислениях.\n",
        "\n",
        "Ранее вы узнали о SparkContext, который является основной точкой входа для создания RDD. Точно так же SparkSession предоставляет единую точку входа для взаимодействия с базовыми функциями Spark и позволяет программировать Spark с помощью DataFrame API.\n",
        "\n",
        "SparkSession делает для DataFrames то же, что SparkContext делает для RDD"
      ],
      "metadata": {
        "id": "TGDZ6k37SxW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# создание DataFrame через RDD или через SparkSession\n",
        "iphones_RDD = sc.parallelize([(\"XS\", 2018, 5.65, 2.79, 6.24), (\"XR\", 2018, 5.94, 2.98, 6.84),\n",
        "                              (\"X10\", 2017, 5.65, 2.79, 6.13), (\"8Plus\", 2017, 6.23, 3.07, 7.12)])\n",
        "names = ['Model','Year', 'Height', 'Width', 'Weight']\n",
        "iphones_df = spark.createDataFrame(iphones_RDD, schema=names)\n",
        "type(iphones_df) # pyspark.sql.dataframe.DataFrame\n",
        "#Схема — это структура данных в DataFrame, которая помогает Spark более эффективно оптимизировать запросы к данным\n",
        "\n",
        "df_csv = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\n",
        "df_json = spark.read.json(\"people.json\", header=True, inferSchema=True)\n",
        "df_txt = spark.read.txt(\"people.txt\", header=True, inferSchema=True)\n",
        "#nferSchema=True дать указание считывателю DataFrame вывести схему из данных и, попытаться назначить правильный тип данных для каждого столбца"
      ],
      "metadata": {
        "id": "7o-aKe1WTjUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_id_age = test.select('Age').show(3)\n",
        "new_df_age21 = new_df.filter(new_df.Age > 21).show(3)\n",
        "test_df_age_group = test_df.groupby('Age').count().orderBy('Age').show(3)\n",
        "test_df_no_dup = test_df.select('User_ID', 'Gender', 'Age').dropDuplicates()\n",
        "test_df_sex = test_df.withColumnRenamed('Gender', 'Sex') # переименовать было стало\n",
        "test_df.printSchema()\n",
        "#    |-- User_ID: integer (nullable = true)\n",
        "#    |-- Product_ID: string (nullable = true)\n",
        "test_df.columns #['User_ID', 'Gender', 'Age'\n",
        "test_df.describe().show()"
      ],
      "metadata": {
        "id": "G15IiiLViGbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL-запросы нельзя запускать напрямую к DataFrame. Чтобы выполнить SQL-запросы к существующему фрейму данных, мы можем использовать функцию createOrReplaceTempView для создания временной таблицы, как показано в этом примере"
      ],
      "metadata": {
        "id": "XLGuBMmOnXKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"table1\")\n",
        "df2 = spark.sql(\"SELECT field1, field2 FROM table1\")\n",
        "df2.collect()"
      ],
      "metadata": {
        "id": "jPGW0MWWnPS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# визуализация либо pyspark_dist_explore library, toPandas(), HandySpark library\n",
        "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
        "test_df_age = test_df.select('Age')\n",
        "hist(test_df_age, bins=20, color=\"red\")\n",
        "# через библиотеку\n",
        "test_df = spark.read.csv('test.csv', header=True, inferSchema=True)\n",
        "hdf = test_df.toHandy()\n",
        "hdf.cols[\"Age\"].hist()"
      ],
      "metadata": {
        "id": "4GkHHfaEpHri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark MLlib"
      ],
      "metadata": {
        "id": "p_l4fRIiujvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.recommendation import ALS # совместной фильтрации для рекоменательных систем\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "from pyspark.mllib.clustering import KMeans\n",
        "#pyspark.mllib - это встроенная библиотека для API на основе RDD."
      ],
      "metadata": {
        "id": "DH9dSsbpukn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# совместная фильтрация\n",
        "from pyspark.mllib.recommendation import Rating\n",
        "r = Rating(user = 1, product = 2, rating = 5.0)\n",
        "(r[0], r[1], r[2]) # (1, 2, 5.0) пользователь, продукт и рейтинг\n",
        "\n",
        "data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) # создаем RDD\n",
        "training, test=data.randomSplit([0.6, 0.4])\n",
        "training.collect()\n",
        "test.collect()\n",
        "# используя алгоритм ALS ищем продукты для клиентов\n",
        "r1 = Rating(1, 1, 1.0)\n",
        "r2 = Rating(1, 2, 2.0)\n",
        "r3 = Rating(2, 1, 2.0)\n",
        "ratings = sc.parallelize([r1, r2, r3])\n",
        "ratings.collect()\n",
        "# [Rating(user=1, product=1, rating=1.0), Rating(user=1, product=2, rating=2.0),\n",
        "# Rating(user=2, product=1, rating=2.0)]\n",
        "model = ALS.train(ratings, rank=10, iterations=10)\n",
        "# прогнозирвание рейтингов для пар пользователь-продукт\n",
        "unrated_RDD = sc.parallelize([(1, 2), (1, 1)])\n",
        "predictions = model.predictAll(unrated_RDD)\n",
        "predictions.collect()\n",
        "#[Rating(user=1, product=1, rating=1.0000278574351853), Rating(user=1, product=2, rating=1.9890355703778122)]\n",
        "rates = ratings.map(lambda x: ((x[0], x[1]), x[2]))\n",
        "rates.collect()\n",
        "  #[((1, 1), 1.0), ((1, 2), 2.0), ((2, 1), 2.0)]\n",
        "preds = predictions.map(lambda x: ((x[0], x[1]), x[2]))\n",
        "preds.collect()\n",
        "  #[((1, 1), 1.0000278574351853), ((1, 2), 1.9890355703778122)]\n",
        "rates_preds = rates.join(preds)\n",
        "rates_preds.collect()\n",
        "  #[((1, 2), (2.0, 1.9890355703778122)), ((1, 1), (1.0, 1.0000278574351853))]\n",
        "MSE = rates_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean() # ошибка MSE"
      ],
      "metadata": {
        "id": "VbhqflK3zst4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "data = sc.textFile(file_path) #['1,31,2.5,1260759144',\n",
        "                              #'1,1029,3.0,1260759179']\n",
        "ratings = data.map(lambda l: l.split(',')) #[['1', '31', '2.5', '1260759144'],\n",
        "                                           # ['1', '1129', '2.0', '1260759185']]\n",
        "\n",
        "ratings_final = ratings.map(lambda line: Rating(int(line[0]),int(line[1]),float(line[2])))#[Rating(user=1, product=31, rating=2.5),\n",
        "                                                                                        #Rating(user=1, product=1129, rating=2.0)]\n",
        "training_data, test_data = ratings_final.randomSplit([0.8, 0.2]) #[Rating(user=1, product=31, rating=2.5),\n",
        "                                                                 #Rating(user=1, product=1029, rating=3.0)]\n",
        "model = ALS.train(training_data, rank=10, iterations=10)\n",
        "testdata_no_rating = test_data.map(lambda p: (p[0], p[1])) # [(1, 1129)]\n",
        "predictions = model.predictAll(testdata_no_rating)\n",
        "predictions.take(2) #[Rating(user=599, product=69069, rating=2.771806602194091),"
      ],
      "metadata": {
        "id": "YOIKXkZW3VcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Классификация\n",
        "\n",
        "Типы данных PySpark MLlib:\n",
        "* Vector\n",
        "\n",
        "> Плотные\n",
        "```\n",
        "denseVec = Vectors.dense([1.0, 2.0, 3.0])\n",
        "DenseVector([1.0, 2.0, 3.0])\n",
        "```\n",
        "> Разреженные\n",
        "```\n",
        "sparseVec = Vectors.sparse(4, {1: 1.0, 3: 5.5})\n",
        "SparseVector(4, {1: 1.0, 3: 5.5})\n",
        "```\n",
        "\n",
        "* LabelledPoint\n",
        "```\n",
        "positive = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
        "negative = LabeledPoint(0.0, [2.0, 1.0, 1.0]) # метка класса и признаки\n",
        "```"
      ],
      "metadata": {
        "id": "cOjLFFG0Yz-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.feature import HashingTF\n",
        "sentence = \"hello hello world\"\n",
        "words = sentence.split()\n",
        "tf = HashingTF(10000)\n",
        "tf.transform(words)\n",
        "# SparseVector(10000, {3065: 1.0, 6861: 2.0}) #кол-во признаков, номер признака и кол-во\n",
        "\n",
        "data = [LabeledPoint(0.0, [0.0, 1.0]), LabeledPoint(1.0, [1.0, 0.0])]\n",
        "RDD = sc.parallelize(data)\n",
        "lrm = LogisticRegressionWithLBFGS.train(RDD)\n",
        "lrm.predict([1.0, 0.0])\n",
        "lrm.predict([0.0, 1.0])"
      ],
      "metadata": {
        "id": "S8sz4z_TaiIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "spam_rdd = sc.textFile(file_path_spam)\n",
        "non_spam_rdd = sc.textFile(file_path_non_spam)\n",
        "\n",
        "spam_words = spam_rdd.flatMap(lambda email: email.split(' ')) #['...', '...' и тд]\n",
        "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
        "\n",
        "tf = HashingTF(numFeatures=200)\n",
        "spam_features = tf.transform(spam_words)\n",
        "non_spam_features = tf.transform(non_spam_words)\n",
        "# [SparseVector(200, {103: 1.0, 111: 1.0, 119: 1.0}),\n",
        "# SparseVector(200, {14: 1.0, 89: 1.0, 193: 1.0, 199: 1.0}),\n",
        "\n",
        "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
        "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
        "#[LabeledPoint(1.0, (200,[103,111,119],[1.0,1.0,1.0])),\n",
        "# LabeledPoint(1.0, (200,[14,89,193,199],[1.0,1.0,1.0,1.0])),\n",
        "\n",
        "samples = spam_samples.join(non_spam_samples)\n",
        "\n",
        "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
        "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
        "predictions = model.predict(test_samples.map(lambda x: x.features)) #[0, 1, 0, 1]\n",
        "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
        "# [(1.0, 0), (1.0, 1), (1.0, 0), (1.0, 0)]\n",
        "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
        "print(\"Model accuracy : {:.2f}\".format(accuracy))"
      ],
      "metadata": {
        "id": "WyXlNpyNaDky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кластеризация"
      ],
      "metadata": {
        "id": "Rb1x1sCLihRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.textFile(\"WineData.csv\").map(lambda x: x.split(\",\")).\\\n",
        "map(lambda x: [float(x[0]), float(x[1])])\n",
        "RDD.take(5) #[[14.23, 2.43], [13.2, 2.14], [13.16, 2.67], [14.37, 2.5], [13.24, 2.87]]\n",
        "\n",
        "from pyspark.mllib.clustering import KMeans\n",
        "model = KMeans.train(RDD, k = 2, maxIterations = 10)\n",
        "model.clusterCenters # центр кластера\n",
        "# [array([12.25573171, 2.28939024]), array([13.636875 , 2.43239583])]\n",
        "\n",
        "from math import sqrt\n",
        "def error(point):\n",
        "    center = model.centers[model.predict(point)]\n",
        "    return sqrt(sum([x**2 for x in (point - center)]))\n",
        "WSSSE = RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
        "print(\"Within Set Sum of Squared Error = \" + str(WSSSE)) #  77.962\n",
        "\n",
        "wine_data_df = spark.createDataFrame(RDD, schema=[\"col1\", \"col2\"])\n",
        "wine_data_df_pandas = wine_data_df.toPandas()\n",
        "cluster_centers_pandas = pd.DataFrame(model.clusterCenters, columns=[\"col1\", \"col2\"])\n",
        "cluster_centers_pandas.head()\n",
        "\n",
        "plt.scatter(wine_data_df_pandas[\"col1\"], wine_data_df_pandas[\"col2\"]);\n",
        "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\"\n",
        "# сначала конвертируем RDD в Spark DataFrame, а затем в Pandas DataFrame\n",
        "# конвертируем кластерные центры из модели KMeans в Pandas DataFrame"
      ],
      "metadata": {
        "id": "GP9d92i8Z-2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning Data with PySpark**"
      ],
      "metadata": {
        "id": "DGy3x89Pnp3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# схема\n",
        "import pyspark.sql.types import *# указывается имя поля, тип, и могут ли быть нулевыми\n",
        "peopleSchema = StructType([StructField('name', StringType(), True),\n",
        "                           StructField('age', IntegerType(), True),\n",
        "                           StructField('city', StringType(), True)])\n",
        "people_df = spark.read.format('csv').load(name='rawdata.csv', schema=peopleSchema)"
      ],
      "metadata": {
        "id": "99Id6wpzTpU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузка и чтение\n",
        "df = spark.read.format('parquet').load('filename.parquet')\n",
        "df = spark.read.parquet('filename.parquet')\n",
        "\n",
        "df.write.format('parquet').save('filename.parquet')\n",
        "df.write.parquet('filename.parquet')"
      ],
      "metadata": {
        "id": "6w0mWdlMZFNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flight_df = spark.read.parquet('flights.parquet') # создаем df\n",
        "flight_df.createOrReplaceTempView('flights') #псевдоним данных Parquet в виде таблицы SQL\n",
        "short_flights_df = spark.sql('SELECT * FROM flights WHERE flightduration < 100')"
      ],
      "metadata": {
        "id": "SQK18UTmascy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df неизменяемые, всегда создается новый df\n",
        "voter_df.filter(voter_df.name.like('M%')) # только те строки с М начинаются\n",
        "voters = voter_df.select('name', 'position')\n",
        "\n",
        "voter_df.filter(voter_df.date > '1/1/2019') # или voter_df.where(...)\n",
        "voter_df.select(voter_df.name)\n",
        "voter_df.withColumn('year', voter_df.date.year)\n",
        "voter_df.drop('unused_column')"
      ],
      "metadata": {
        "id": "TVCmTAn7gVyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voter_df.filter(voter_df['name'].isNotNull()) # удалание пустых значений\n",
        "voter_df.filter(voter_df.date.year > 1800)\n",
        "voter_df.where(voter_df['_c0'].contains('VOTE'))\n",
        "voter_df.where(~ voter_df._c1.isNull())\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "voter_df.withColumn('upper', F.upper('name'))\n",
        "\n",
        "voter_df.withColumn('splits', F.split('name',' ')) # столбец списков\n",
        "\n",
        "voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))\n",
        "\n",
        "# работа с ArrayType() - список\n",
        ".size() # кол-во элементов\n",
        ".getItem(<index>) # элемент в индексе"
      ],
      "metadata": {
        "id": "Xp5dwK50h5rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
        "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
        "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
        "voter_df = voter_df.drop('splits')\n",
        "voter_df.show()"
      ],
      "metadata": {
        "id": "K9CpLDqIosl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df.Name, df.Age, F.when(df.Age >= 18, \"Adult\")) #из pyspark.sql.functions\n",
        "+-----+----+------+\n",
        "| Name| Age|      |\n",
        "+-----+----+------+\n",
        "|Alice|  14|      |\n",
        "|  Bob|  18| Aduit|\n",
        "+-----+----+------+\n",
        "df.select(df.Name, df.Age, when(df.Age >= 18, \"Adult\").when(df.Age < 18, \"Minor\"))\n",
        "# иначе\n",
        "df.select(df.Name, df.Age, when(df.Age >= 18, \"Adult\").otherwise(\"Minor\"))"
      ],
      "metadata": {
        "id": "UzRq-8aa7KaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пользовательские функции UDF\n",
        "import pyspark.sql.functions.udf\n",
        "def reverseString(mystr):\n",
        "    return mystr[::-1]\n",
        "udfReverseString = udf(reverseString, StringType()) #обернуть функцию и сохранить ее в переменной\n",
        "# имя метода и тип данных Spark для возврата\n",
        "user_df = user_df.withColumn('ReverseName', udfReverseString(user_df.Name))\n",
        "# функция применяется для каждой строки\n",
        "\n",
        "def sortingCap():\n",
        "    return random.choice(['G', 'H', 'R', 'S'])\n",
        "udfSortingCap = udf(sortingCap, StringType())\n",
        "user_df = user_df.withColumn('Class', udfSortingCap()) # случайно возвращает букву\n",
        "\n",
        "# пример с сортировкой\n",
        "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
        "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
        "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
      ],
      "metadata": {
        "id": "nJph9coICAdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())# кол-во разделов\n",
        "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
        "\n",
        "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
        "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
        "\n",
        "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
        "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
      ],
      "metadata": {
        "id": "L9Dt1ceeLQXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
        "\n",
        "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
        "voter_df_march.select('ROW_ID').show()\n",
        "voter_df_april.select('ROW_ID').show()"
      ],
      "metadata": {
        "id": "nZIgEE4aqcBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кэширование"
      ],
      "metadata": {
        "id": "4IKb04gmtqOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voter_df = spark.read.csv('voter_data.txt.gz') # создание df\n",
        "voter_df.cache().count() # кэш трансформер\n",
        "# либо кэшировать\n",
        "voter_df = voter_df.withColumn('ID', monotonically_increasing_id())\n",
        "voter_df = voter_df.cache() # для кэширования\n",
        "voter_df.show() # тут уже кэшируется\n",
        "print(voter_df.is_cached) # кэширование\n",
        "voter_df.unpersist() # отмена кэширования"
      ],
      "metadata": {
        "id": "lmy5nPPYtuJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "departures_df = departures_df.distinct().cache()\n",
        "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
        "\n",
        "start_time = time.time() # сброс времени\n",
        "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
        "# Counting 139358 rows took 4.620641 seconds\n",
        "# Counting 139358 rows again took 0.905145 seconds"
      ],
      "metadata": {
        "id": "EBlXz8Ksv7id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фишки импорта"
      ],
      "metadata": {
        "id": "LGvG2HzOy1Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "airport_df = spark.read.csv('airports-*.txt.gz') # один оператор импорта, файлов несколько"
      ],
      "metadata": {
        "id": "wGySjUqay3H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.csv('singlelargefile.csv')\n",
        "df_csv.write.parquet('data.parquet')\n",
        "df = spark.read.parquet('data.parquet')\n",
        "\n",
        "# Пример\n",
        "full_df = spark.read.csv('departures_full.txt.gz')\n",
        "split_df = spark.read.csv('departures_0*.txt.gz') # разбитый\n",
        "\n",
        "start_time_a = time.time()\n",
        "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
        "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
        "\n",
        "start_time_b = time.time()\n",
        "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
        "print(\"Time to run: %f\" % (time.time() - start_time_b))"
      ],
      "metadata": {
        "id": "8GjdKQVU7i6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.get(<configuration name>) # для чтения параметра конфигурации с именем\n",
        "spark.conf.set(<configuration name>) # для записи параметра\n",
        "# пример\n",
        "app_name = spark.conf.get('spark.app.name') # Name: pyspark-shell\n",
        "driver_tcp_port = spark.conf.get('spark.driver.port') #  Driver TCP port: 32931\n",
        "num_partitions = spark.conf.get('spark.sql.shuffle.partitions') # Number of partitions: 200\n",
        "\n",
        "# пример\n",
        "# Store the number of partitions in variable\n",
        "before = departures_df.rdd.getNumPartitions()\n",
        "# Configure Spark to use 500 partitions\n",
        "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
        "# Recreate the DataFrame using the departures data file\n",
        "departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
        "# Print the number of partitions for each instance\n",
        "print(\"Partition count before change: %d\" % before) # 1\n",
        "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions()) # 1"
      ],
      "metadata": {
        "id": "vj7543krU6sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voter_df = df.select(df['VOTER NAME']).distinct()\n",
        "voter_df.explain() # план, который будет запущен для получения результатов\n"
      ],
      "metadata": {
        "id": "DIxqS0TifdHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# перетасовка\n",
        "# Limit use of .repartition(num_partitions)\n",
        "# Use .coalesce(num_partitions) instead\n",
        "# Use care when calling .join()\n",
        "# Use .broadcast()\n",
        "# May not need to limit i\n",
        "\n",
        "# Broadcasting\n",
        "from pyspark.sql.functions import broadcast\n",
        "combined_df = df_1.join(broadcast(df_2))"
      ],
      "metadata": {
        "id": "10yeuoHRhCu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "# Import the data to a DataFrame\n",
        "departures_df = spark.read.csv('2015-departures.csv.gz', header=True)\n",
        "\n",
        "# Remove any duration of 0\n",
        "departures_df = departures_df.filter(departures_df[3] > 0)\n",
        "\n",
        "# Add an ID column\n",
        "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
        "\n",
        "# Write the file out to JSON format\n",
        "departures_df.write.json('output.json', mode='overwrite')"
      ],
      "metadata": {
        "id": "8Qc8A3WZmF09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# продолжи"
      ],
      "metadata": {
        "id": "3o1HuGe5HnYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering with PySpark**"
      ],
      "metadata": {
        "id": "TqJu-Z3PHqOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.version # версия спарк тут version 2.3.1\n",
        "import sys # версия питона\n",
        "sys.version_info\n",
        "\n",
        "spark.read.json('example.json') # всё в df\n",
        "spark.read.csv('example.csv')\n",
        "spark.read.parquet('example.parq')\n",
        "df = spark.read.parquet('example.parq')\n",
        "\n",
        "df.count()\n",
        "df.columns\n",
        "len(df.columns)\n",
        "df.dtypes # [(название столбца, тип)...]\n",
        "\n",
        "df.describe(['LISTPRICE']).show() # можно для одного, всех или списка\n",
        "df.agg({'SALESCLOSEPRICE': 'mean'}).collect()#[Row(avg(SALESCLOSEPRICE)=262804.4668)]\n",
        "# так как mean агрегатная функция\n",
        "# пример\n",
        "from pyspark.sql.functions import skewness\n",
        "print(df.agg({'LISTPRICE': 'skewness'}).collect())\n",
        "\n",
        "df.cov('SALESCLOSEPRICE', 'YEARBUILT') # 1281910.384\n",
        "\n",
        "# для использования seaborn надо преобразовать df PySpark в pandas PySpark но могут быть сбои\n",
        "# берем подвыборку\n",
        "df.sample(False, 0.5, 42).count() # без повторения, объем, начальное зерно\n",
        "# тип pyspark.sql.dataframe.DataFrame\n",
        "import seaborn as sns\n",
        "sample_df = df.select(['SALESCLOSEPRICE']).sample(False, 0.5, 42)\n",
        "pandas_df = sample_df.toPandas()\n",
        "sns.distplot(pandas_df)\n",
        "\n",
        "import seaborn as sns\n",
        "s_df = df.select(['SALESCLOSEPRICE', 'SQFTABOVEGROUND'])\n",
        "s_df = s_df.sample(False, 0.5, 42)\n",
        "pandas_df = s_df.toPandas()\n",
        "sns.lmplot(x='SQFTABOVEGROUND', y='SALESCLOSEPRICE', data=pandas_df)"
      ],
      "metadata": {
        "id": "x-1yVVDxH2pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = ['NO', 'UNITNUMBER', 'CLASS']\n",
        "df = df.drop(*cols_to_drop)\n",
        "df = df.where(~df['POTENTIALSHORTSALE'].like('Not Disclosed'))\n",
        "\n",
        "# удаление выбросов\n",
        "std_val = df.agg({'SALESCLOSEPRICE': 'stddev'}).collect()[0][0]\n",
        "mean_val = df.agg({'SALESCLOSEPRICE': 'mean'}).collect()[0][0]\n",
        "hi_bound = mean_val + (3 * std_val)\n",
        "low_bound = mean_val - (3 * std_val)\n",
        "df = df.where((df['LISTPRICE'] < hi_bound) & (df['LISTPRICE'] > low_bound))\n",
        "# удаление пропущенных\n",
        "df = df.dropna() # либо хотя бы где-то есть\n",
        "df = df.dropna(how='all', subset['LISTPRICE', 'SALESCLOSEPRICE ']) # по столбцам\n",
        "df = df.dropna(thresh=2) # как минимум в двух столбцах\n",
        "\n",
        "# удаление дубликатов\n",
        "df.dropDuplicates()\n",
        "df.dropDuplicates(['streetaddress'])"
      ],
      "metadata": {
        "id": "vGkF_eq-eS0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# практика\n",
        "df.select(['ASSUMABLEMORTGAGE']).distinct().show()\n",
        "yes_values = ['Yes w/ Qualifying', 'Yes w/No Qualifying']\n",
        "text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
        "df = df.where(text_filter)\n",
        "print(df.count())"
      ],
      "metadata": {
        "id": "QxOMxVpmo57g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min-max scaling\n",
        "# define min and max values and collect them\n",
        "max_days = df.agg({'DAYSONMARKET': 'max'}).collect()[0][0]\n",
        "min_days = df.agg({'DAYSONMARKET': 'min'}).collect()[0][0]\n",
        "# create a new column based off the scaled data\n",
        "df = df.withColumn(\"scaled_days\", (df['DAYSONMARKET']-min_days)/(max_days-min_days))\n",
        "df[['scaled_days']].show(5)\n",
        "\n",
        "# стандартизация\n",
        "mean_days = df.agg({'DAYSONMARKET': 'mean'}).collect()[0][0]\n",
        "stddev_days = df.agg({'DAYSONMARKET': 'stddev'}).collect()[0][0]\n",
        "df = df.withColumn(\"ztrans_days\", (df['DAYSONMARKET']-mean_days)/stddev_days)\n",
        "df.agg({'ztrans_days': 'mean'}).collect()\n",
        "df.agg({'ztrans_days': 'stddev'}).collect()\n",
        "\n",
        "#log-scaling\n",
        "from pyspark.sql.functions import log\n",
        "df = df.withColumn('log_SalesClosePrice', log(df['SALESCLOSEPRICE']))"
      ],
      "metadata": {
        "id": "P53a1RKDtAlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(df['ROOF'].isNull()).count() # подсчет пропущенных\n",
        "# построение тепловой карты\n",
        "import seaborn as sns\n",
        "sub_df = df.select(['ROOMAREA1'])\n",
        "sample_df = sub_df.sample(False, .5, 4)\n",
        "pandas_df = sample_df.toPandas()\n",
        "sns.heatmap(data=pandas_df.isnull()) # для True False\n",
        "\n",
        "# заполенение пропущенных\n",
        "df.fillna(0, subset=['DAYSONMARKET'])\n",
        "\n",
        "col_mean = df.agg({'DAYSONMARKET': 'mean'}).collect()[0][0]\n",
        "df.fillna(col_mean, subset=['DAYSONMARKET'])"
      ],
      "metadata": {
        "id": "U0N83qMHyQAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# присоединение\n",
        "DataFrame.join(other, on=None, how=None)\n",
        "\n",
        "cond = [df['OFFMARKETDATE'] == hdf['dt']]\n",
        "df = df.join(hdf, on=cond, 'left')\n",
        "df.where(~df['nm'].isNull()).count()\n",
        "# через SQL запрос\n",
        "df.createOrReplaceTempView(\"df\")\n",
        "hdf.createOrReplaceTempView(\"hdf\")\n",
        "sql_df = spark.sql(\"\"\"SELECT*\n",
        "                      FROM df\n",
        "                      LEFT JOIN hdf ON df.OFFMARKETDATE = hdf.dt\"\"\")"
      ],
      "metadata": {
        "id": "onczeoo902LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "def min_max_scaler(df, cols_to_scale):\n",
        "  for col in cols_to_scale:\n",
        "    max_days = df.agg({col: 'max'}).collect()[0][0]\n",
        "    min_days = df.agg({col: 'min'}).collect()[0][0]\n",
        "    new_column_name = 'scaled_' + col\n",
        "    df = df.withColumn(new_column_name,(df[col]-min_days)/(max_days-min_days))\n",
        "  return df\n",
        "\n",
        "df = min_max_scaler(df, cols_to_scale)\n",
        "df[['DAYSONMARKET', 'scaled_DAYSONMARKET']].show()\n",
        "\n",
        "# пример\n",
        "walk_df = walk_df.withColumn('longitude', walk_df['longitude'].cast('double'))\n",
        "walk_df = walk_df.withColumn('latitude', walk_df['latitude'].cast('double'))\n",
        "\n",
        "df = df.withColumn('longitude', round(df['longitude'], 5))\n",
        "df = df.withColumn('latitude', round(df['latitude'], 5))\n",
        "\n",
        "condition = [(df['longitude'] == walk_df['longitude']), (df['latitude'] == walk_df['latitude'])]\n",
        "\n",
        "join_df = df.join(walk_df, on=condition, how='left')\n",
        "print(join_df.where(~join_df['walkscore'].isNull()).count())"
      ],
      "metadata": {
        "id": "Q6t0DIab52W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Генериование признаков"
      ],
      "metadata": {
        "id": "3oo203k-jX5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('TSQFT', (df['WIDTH'] * df['LENGTH']))\n",
        "df = df.withColumn('TSQFT', (df['SQFTBELOWGROUND'] + df['SQFTABOVEGROUND']))\n",
        "df = df.withColumn('PRICEPERTSQFT', (df['LISTPRICE'] / df['TSQFT']))\n",
        "df = df.withColumn('DAYSONMARKET', datediff('OFFMARKETDATE', 'LISTDATE'))"
      ],
      "metadata": {
        "id": "JVWLhOAxjaGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date # либо to_timestamp для даты Spark\n",
        "df = df.withColumn('LISTDATE', to_date('LISTDATE'))\n",
        "df[['LISTDATE']].show(2)\n",
        "\n",
        "from pyspark.sql.functions import year, month\n",
        "df = df.withColumn('LIST_YEAR', year('LISTDATE'))\n",
        "df = df.withColumn('LIST_MONTH', month('LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import dayofmonth, weekofyear\n",
        "df = df.withColumn('LIST_DAYOFMONTH', dayofmonth('LISTDATE'))\n",
        "df = df.withColumn('LIST_WEEKOFYEAR', weekofyear('LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import datediff\n",
        "df.withColumn('DAYSONMARKET', datediff('OFFMARKETDATE', 'LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import lag\n",
        "from pyspark.sql.window import Window\n",
        "w = Window().orderBy(m_df['DATE'])\n",
        "m_df = m_df.withColumn('MORTGAGE-1wk', lag('MORTGAGE', count=1).over(w))\n",
        "m_df.show(3)\n",
        "+----------+------------+----------------+\n",
        "|      DATE|    MORTGAGE|    MORTGAGE-1wk|\n",
        "+----------+------------+----------------+\n",
        "|2013-10-10|        4.23|            null|\n",
        "|2013-10-17|        4.28|            4.23|\n",
        "|2013-10-24|        4.13|            4.28|\n",
        "+----------+------------+----------------+"
      ],
      "metadata": {
        "id": "jSjn5haVpOfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "find_under_8 = df['ROOF'].like('%Age 8 Years or Less%')\n",
        "find_over_8 = df['ROOF'].like('%Age Over 8 Years%')\n",
        "\n",
        "df = df.withColumn('old_roof', (when(find_over_8, 1).when(find_under_8, 0).otherwise(None)))\n",
        "df[['ROOF', 'old_roof']].show(3, truncate=100)\n",
        "\n",
        "from pyspark.sql.functions import split\n",
        "split_col = split(df['ROOF'], ',')\n",
        "\n",
        "df = df.withColumn('Roof_Material', split _col.getItem(0))\n",
        "df[['ROOF', 'Roof_Material']].show(5, truncate=100)"
      ],
      "metadata": {
        "id": "g0L1oPAy7U27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# из этого\n",
        "+---+------------------------------------------------+\n",
        "|NO |                                       roof_list|\n",
        "+---+------------------------------------------------+\n",
        "| 2 |  Asphalt Shingles, Pitched, Age 8 Years or Less|\n",
        "# в это (пивот)\n",
        "+---+--------------------+\n",
        "|NO |        ex_roof_list|\n",
        "+---+--------------------+\n",
        "|  2|    Asphalt Shingles|\n",
        "|  2|             Pitched|\n",
        "|  2| Age 8 Years or Less|\n",
        "\n",
        "from pyspark.sql.functions import split, explode, lit, coalesce, first\n",
        "df = df.withColumn('roof_list', split(df['ROOF'], ',')) #в столбец списка\n",
        "ex_df = df.withColumn('ex_roof_list', explode(df['roof_list'])) # список в один столбец\n",
        "# Create a dummy column of constant value\n",
        "ex_df = ex_df.withColumn('constant_val', lit(1))\n",
        "# Pivot the values into boolean columns\n",
        "piv_df = ex_df.groupBy('NO').pivot('ex_roof_list').agg(coalesce(first('constant_val')))\n",
        "\n",
        "#пример\n",
        "from pyspark.sql.functions import coalesce, first\n",
        "# Pivot\n",
        "piv_df = ex_df.groupBy('NO').pivot('ex_garage_list').agg(coalesce(first('constant_val')))\n",
        "joined_df = df.join(piv_df, on='NO', how='left')\n",
        "zfill_cols = piv_df.columns\n",
        "zfilled_df = joined_df.fillna(0, subset=zfill_cols) # таблица + one-hot для слов в списке"
      ],
      "metadata": {
        "id": "pc-ThiS3Xoiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# бинаризация\n",
        "from pyspark.ml.feature import Binarizer\n",
        "# важно чтобы тип был double\n",
        "df = df.withColumn('FIREPLACES', df['FIREPLACES'].cast('double'))\n",
        "# Create binarizing transformer\n",
        "bin = Binarizer(threshold=0.0, inputCol='FIREPLACES', outputCol='FireplaceT')\n",
        "df = bin.transform(df) # все что выше 0.0 -> 1\n",
        "+----------+-------------+\n",
        "|FIREPLACES|   FireplaceT|\n",
        "+----------+-------------+\n",
        "|       0.0|          0.0|\n",
        "|       1.0|          1.0|\n",
        "|       2.0|          1.0|\n",
        "\n",
        "#Bucketing (сегментирование, биннинг, порядковые переменные)\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "splits = [0, 1, 2, 3, 4, float('Inf')]\n",
        "# Create bucketing transformer\n",
        "buck = Bucketizer(splits=splits, inputCol='BATHSTOTAL', outputCol='baths')\n",
        "df = buck.transform(df)\n",
        "df[['BATHSTOTAL', 'baths']].show(4)\n",
        "# от 0 до 1 -> 1, от 1 до 2 -> 2, от 2 до 3 -> 3\n",
        "\n",
        "#one-hot кодирование\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "# Create indexer transformer (сопоставление слова с числом)\n",
        "stringIndexer = StringIndexer(inputCol='CITY', outputCol='City_Index')\n",
        "# Fit transformer\n",
        "model = stringIndexer.fit(df)\n",
        "# Apply transformer\n",
        "indexed = model.transform(df)\n",
        "\n",
        "encoder = OneHotEncoder(inputCol='City_Index', outputCol='City_Vec')\n",
        "# Apply the encoder transformer\n",
        "encoded_df = encoder.transform(indexed)\n",
        "encoded_df[['City_Vec']].show(4) # без последней категории\n",
        "+-------------+\n",
        "|     City_Vec|\n",
        "+-------------+\n",
        "|    (4,[],[])|\n",
        "|    (4,[],[])|\n",
        "|(4,[2],[1.0])|\n",
        "|(4,[2],[1.0])|\n",
        "+-------------+\n"
      ],
      "metadata": {
        "id": "CSIb4KL5nlJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Машинное обучение с PySpark\n",
        "\n",
        "ml.regression\n",
        "\n",
        "\n",
        "*   GeneralizedLinearRegression\n",
        "*   IsotonicRegression\n",
        "*   LinearRegression\n",
        "*   DecisionTreeRegression\n",
        "*   GBTRegression (деревья с повышением градиента)\n",
        "*   RandomForestRegression\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3U_qNIvEwSwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_date = df.agg({'OFFMKTDATE': 'max'}).collect()[0][0]\n",
        "min_date = df.agg({'OFFMKTDATE': 'min'}).collect()[0][0]\n",
        "from pyspark.sql.functions import datediff\n",
        "range_in_days = datediff(max_date, min_date)\n",
        "# Find the date to split the dataset on\n",
        "from pyspark.sql.functions import date_add\n",
        "split_in_days = round(range_in_days * 0.8)\n",
        "split_date = date_add(min_date, split_in_days)\n",
        "# Split the data into 80% train, 20% test\n",
        "train_df = df.where(df['OFFMKTDATE'] < split_date)\n",
        "test_df = df.where(df['OFFMKTDATE'] >= split_date).where(df['LISTDATE'] >= split_date)"
      ],
      "metadata": {
        "id": "Hm_FtkLSwXAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример с lit (откуда lit)\n",
        "from pyspark.sql.functions import datediff, to_date, lit\n",
        "split_date = to_date(lit('2017-12-10'))\n",
        "test_df = df.where(df['OFFMKTDATE'] >= split_date).where(df['LISTDATE'] <= split_date)\n",
        "test_df = test_df.withColumn('DAYSONMARKET_Original', test_df['DAYSONMARKET'])\n",
        "test_df = test_df.withColumn('DAYSONMARKET', datediff(split_date, 'LISTDATE'))"
      ],
      "metadata": {
        "id": "XBsuYJ3i4Lw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df.count(), len(df.columns))) # (5000, 126)\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "# все признаки в одном столбце, но VectorAssembler не умеет работать с NaN\n",
        "df = df.fillna(-1)\n",
        "features_cols = list(df.columns)\n",
        "features_cols.remove('SALESCLOSEPRICE')\n",
        "\n",
        "vec = VectorAssembler(inputCols=features_cols, outputCol='features')\n",
        "df = vec.transform(df)\n",
        "ml_ready_df = df.select(['SALESCLOSEPRICE', 'features'])\n",
        "ml_ready_df.show(5)\n",
        "+----------------+--------------------+\n",
        "| SALESCLOSEPRICE|            features|\n",
        "+----------------+--------------------+\n",
        "|         143000 |(125,[0,1,2,3,5,6...|\n",
        "|         190000 |(125,[0,1,2,3,5,6...|\n",
        "|         225000 |(125,[0,1,2,3,5,6...|"
      ],
      "metadata": {
        "id": "YRJ9qaXPEy5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "df = df.fillna(-1, subset=['WALKSCORE', 'BIKESCORE'])\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_IDX\").setHandleInvalid(\"keep\") for col in categorical_cols]\n",
        "indexer_pipeline = Pipeline(stages=indexers)\n",
        "df_indexed = indexer_pipeline.fit(df).transform(df)\n",
        "\n",
        "df_indexed = df_indexed.drop(*categorical_cols)\n",
        "print(df_indexed.dtypes)\n",
        "# категории в числа в своих столбцах"
      ],
      "metadata": {
        "id": "J1eKRURuKeuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Отсутствующие значения обрабатываются случайными лесами внутри системы, где они разделяются на отсутствующие значения. До тех пор, пока вы заменяете их чем-то, выходящим за пределы диапазона нормальных значений, они будут обрабатываться правильно. Аналогично, категориальные объекты нужно сопоставлять только с числами, все они могут оставаться в одном столбце с помощью StringIndexer, как мы видели в главе 3. Кодировка OneHot, которая преобразует каждое возможное значение в его собственную логическую функцию, не требуется."
      ],
      "metadata": {
        "id": "25I9lpVhL_rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"SALESCLOSEPRICE\",\n",
        "                           predictionCol=\"Prediction_Price\", seed=42)\n",
        "# столбец признаков, лейблов, выхода и зерно\n",
        "model = rf.fit(train_df)\n",
        "predictions = model.transform(test_df)\n",
        "predictions.select(\"Prediction_Price\", \"SALESCLOSEPRICE\").show(5)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"SALESCLOSEPRICE\", predictionCol=\"Prediction_Price\")\n",
        "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
        "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
        "print('RMSE: ' + str(rmse)) # есть необъяснимая дисперсия\n",
        "print('R^2: ' + str(r2))"
      ],
      "metadata": {
        "id": "oQRUQewdNDSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# важность признака\n",
        "import pandas as pd\n",
        "fi_df = pd.DataFrame(model.featureImportances.toArray(), columns=['importance'])\n",
        "fi_df['feature'] = pd.Series(feature_cols)\n",
        "fi_df.sort_values(by=['importance'], ascending=False, inplace=True)\n",
        "model_df.head(9)\n",
        "|                 feature |importance|\n",
        "|-------------------------|----------|\n",
        "|               LISTPRICE | 0.312101 |\n",
        "|       ORIGINALLISTPRICE | 0.202142 |\n",
        "|              LIVINGAREA | 0.124239 |\n",
        "\n",
        "model.save('rfr_real_estate_model') # сохранение модели, тут целый каталог файлов\n",
        "from pyspark.ml.regression import RandomForestRegressionModel\n",
        "# для загрузки\n",
        "model2 = RandomForestRegressionModel.load('rfr_real_estate_model')"
      ],
      "metadata": {
        "id": "EZLlDcfpYt5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning with PySpark**"
      ],
      "metadata": {
        "id": "ouUxYfx7h8pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark #делает функциональность Spark доступной в интерпретаторе Python\n",
        "pyspark.__version__ # '2.4.1'\n",
        "# подмодули\n",
        "pyspark.sql #Structured Data\n",
        "pyspark.streaming #Streaming Data\n",
        "pyspark.mllib #Machine Learning на RDD (устаревшая)\n",
        "pyspark.ml # текущее устаревшее на DataFrame"
      ],
      "metadata": {
        "id": "0OZvJsrth-MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подключение к Spark - указать Spark где находится кластер через **spark://<IP address | DNS name>:<port>** Пример: spark://13.59.151.161:7077 т.е указать URL-адрес Spark, который указывает сетевое расположение главного узла кластера. URL-адрес состоит из IP-адреса или DNS-имени и номера порта. Порт по умолчанию для Spark — 7077, но его все равно необходимо указать явно.\n",
        "\n",
        "Но мы создадим локальный кластер с указанием количества ядер: local - одно, local[4] - 4, local[*] - все доступные"
      ],
      "metadata": {
        "id": "meiC9xWSDuQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').appName('first_spark_application').getOrCreate()\n",
        "# подключение к Spark создавая объект SparkSession, указывая расположение и имя сессии\n",
        "spark.stop() # остановка SparkSession\n",
        "\n",
        "cars = spark.read.csv('cars.csv', header=True) # параметры header, sep, schema и inferSchema\n",
        "# nullValue заполнитель отсутсвующих данных\n",
        "cars.printSchema()\n",
        "# root\n",
        "# |-- mfr: string (nullable = true)\n",
        "# |-- mod: string (nullable = true)\n",
        "# |-- org: string (nullable = true)\n",
        "cars = spark.read.csv(\"cars.csv\", header=True, inferSchema=True)\n",
        "cars.dtypes # даем автоматически определить тип\n",
        "\n",
        "cars = spark.read.csv(\"cars.csv\", header=True, inferSchema=True, nullValue='NA')\n",
        "# 'NA' будет как NaN\n",
        "schema = StructType([StructField(\"maker\", StringType()), StructField(\"model\", StringType()),\n",
        "                      StructField(\"origin\", StringType()), StructField(\"type\", StringType()),\n",
        "                      StructField(\"cyl\", IntegerType()), StructField(\"size\", DoubleType()),\n",
        "                      StructField(\"weight\", IntegerType()), StructField(\"length\", DoubleType()),\n",
        "                      StructField(\"rpm\", IntegerType()), StructField(\"consumption\", DoubleType())])\n",
        "cars = spark.read.csv(\"cars.csv\", header=True, schema=schema, nullValue='NA')\n",
        "# Это также позволяет выбирать альтернативные имена столбцов"
      ],
      "metadata": {
        "id": "ZqxRn4ehEEHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Классификация (дерево и логистическая)"
      ],
      "metadata": {
        "id": "mt9IEEhIXixs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# практика\n",
        "from pyspark.sql.functions import round\n",
        "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)).drop('mile')\n",
        "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
        "# из True False в 1 и 0"
      ],
      "metadata": {
        "id": "cxxiOG1v7Xhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cars = cars.drop('maker', 'model') # либо удалить либо выбрать\n",
        "cars = cars.select('origin', 'type', 'cyl', 'size', 'weight', 'length', 'rpm', 'consumption')\n",
        "\n",
        "cars.filter('cyl IS NULL').count() # подсчет пустых\n",
        "cars = cars.filter('cyl IS NOT NULL') # либо прям удаление\n",
        "cars = cars.dropna()\n",
        "\n",
        "from pyspark.sql.functions import round\n",
        "cars = cars.withColumn('mass', round(cars.weight / 2.205, 0))\n",
        "cars = cars.withColumn('length', round(cars.length * 0.0254, 3))\n",
        "\n",
        "# категории в числа\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol='type', outputCol='type_idx')\n",
        "# чем чаще тем ближе к 0, контроль через stringOrderType\n",
        "indexer = indexer.fit(cars)\n",
        "cars = indexer.transform(cars)\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['cyl', 'size'], outputCol='features')\n",
        "assembler.transform(cars)\n",
        "+---+----+---------+\n",
        "|cyl|size| features|\n",
        "+---+----+---------+\n",
        "|  3| 1.0|[3.0,1.0]|\n",
        "|  4| 1.3|[4.0,1.3]|\n",
        "# разеделение на обучение и тест\n",
        "cars_train, cars_test = cars.randomSplit([0.8, 0.2], seed=23)"
      ],
      "metadata": {
        "id": "c33W6ZPiXmFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier()\n",
        "tree_model = tree.fit(cars_train)\n",
        "prediction = tree_model.transform(cars_test) # добавляет новые столбцы\n",
        "+-----+----------+---------------------------------------+\n",
        "|label|prediction|                           probability |\n",
        "+-----+----------+---------------------------------------+\n",
        "| 1.0 |      0.0 |[0.9615384615384616,0.0384615384615385]|\n",
        "# 0 с вероятностью 0.961\n",
        "prediction.groupBy(\"label\", \"prediction\").count().show()\n",
        "+-----+----------+-----+\n",
        "|label|prediction|count|\n",
        "+-----+----------+-----+\n",
        "|  1.0|       1.0|    8| <- True positive (TP)\n",
        "|  0.0|       1.0|    2| <- False positive (FP)\n",
        "|  1.0|       0.0|    3| <- False negative (FN)\n",
        "|  0.0|       0.0|    6| <- True negative (TN)"
      ],
      "metadata": {
        "id": "-s7KghGbcYIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "logistic = LogisticRegression()\n",
        "logistic = logistic.fit(cars_train)\n",
        "prediction = logistic.transform(cars_test)\n",
        "+-----+----------+---------------------------------------+\n",
        "|label|prediction|                           probability |\n",
        "+-----+----------+---------------------------------------+\n",
        "| 0.0 |      0.0 |[0.8683802216422138,0.1316197783577862]|\n",
        "| 0.0 |      1.0 |[0.1343792056399585,0.8656207943600416]|\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator()\n",
        "evaluator.evaluate(prediction, {evaluator.metricName: 'weightedPrecision'})\n",
        "# другие метрики weightedRecall, accuracy, f1"
      ],
      "metadata": {
        "id": "E1zy8OiOd_JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "#Дефис экранируется обратной косой чертой, потому что он имеет другое значение в контексте регулярных выражений.\n",
        "# Убегая от него, вы указываете Spark буквально интерпретировать дефис\n",
        "REGEX = '[,\\\\-]'\n",
        "books = books.withColumn('text', regexp_replace(books.text, REGEX, ' '))\n",
        "# из   Forever, or a Long, Long Time   в   Forever or a Long Long Time\n",
        "from pyspark.ml.feature import Tokenizer # список токенов и дополнительно слова в нижнем регистре\n",
        "books = Tokenizer(inputCol=\"text\", outputCol=\"tokens\").transform(books)\n",
        "\n",
        "from pyspark.ml.feature import StopWordsRemover # удаление стоп слов\n",
        "stopwords = StopWordsRemover()\n",
        "stopwords.getStopWords() # получаем список стоп-слов\n",
        "stopwords = stopwords.setInputCol('tokens').setOutputCol('words') # можно было и обычно\n",
        "books = stopwords.transform(books)"
      ],
      "metadata": {
        "id": "T5lONLkvy-JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF\n",
        "hasher = HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=32)\n",
        "books = hasher.transform(books)\n",
        "#из [forever, long, long, time]  в  (32,[8,13,14],[2.0,1.0,1.0]) кол-во признаков, хэши, кол-во раз\n",
        "# после можно получить tf-idf представление\n",
        "from pyspark.ml.feature import IDF\n",
        "books = IDF(inputCol=\"hash\", outputCol=\"features\").fit(books).transform(books)\n",
        "#[forever, long, long, time] (32,[8,13,14],[2.598,1.299,1.704])"
      ],
      "metadata": {
        "id": "iMrw8Pah3snP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Регрессия"
      ],
      "metadata": {
        "id": "MZ9aXFe_dQP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#необходимо преобразовать значения индекса в формат, в котором можете выполнять значимые математические операции\n",
        "from pyspark.ml.feature import OneHotEncoder # one-hot\n",
        "onehot = OneHotEncoder(inputCols=['type_idx'], outputCols=['type_dummy'])\n",
        "onehot = onehot.fit(cars)\n",
        "onehot.categorySizes # [6]\n",
        "cars = onehot.transform(cars)\n",
        "cars.select('type', 'type_idx', 'type_dummy').distinct().sort('type_idx').show()\n",
        "+-------+--------+-------------+\n",
        "|   type|type_idx|   type_dummy|\n",
        "+-------+--------+-------------+\n",
        "|Midsize|     0.0|(5,[0],[1.0])| # разреженный формат и всего 5 категорий\n",
        "|  Small|     1.0|(5,[1],[1.0])| # последняя не входит\n",
        "\n",
        "from pyspark.mllib.linalg import DenseVector, SparseVector\n",
        "DenseVector([1, 0, 0, 0, 0, 7, 0, 0]) # плотное представление\n",
        "# DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0])\n",
        "SparseVector(8, [0, 5], [1, 7]) # кол-во, позиция, и значения\n",
        "# SparseVector(8, {0: 1.0, 5: 7.0})"
      ],
      "metadata": {
        "id": "6RHhPCcidRfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bucketing\n",
        "#Результирующая категориальная переменная часто является более мощным предиктором, чем исходная непрерывная переменная\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "bucketizer = Bucketizer(splits=[3500, 4500, 6000, 6500], inputCol=\"rpm\", outputCol=\"rpm_bin\")\n",
        "bucketed = bucketizer.transform(cars)\n",
        "bucketed.select('rpm', 'rpm_bin').show(5)\n",
        "bucketed.groupBy('rpm_bin').count().show()"
      ],
      "metadata": {
        "id": "0NTaeW8VmYQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=['mass','cyl','type_dummy','density_line','density_quad','density_cube'], outputCol='features')\n",
        "cars = assembler.transform(cars)\n",
        "+-----------------------------------------------------------------------------+-----------+\n",
        "|features                                                                     |consumption|\n",
        "+-----------------------------------------------------------------------------+-----------+\n",
        "|[1451.0,6.0,1.0,0.0,0.0,0.0,0.0,303.8743455497,63.63860639785,13.32745683724]| 9.05 |\n",
        "\n",
        "regression = LinearRegression(labelCol='consumption').fit(cars_train)\n",
        "regression.coefficients # DenseVector([-0.012, 0.174,-0.897,-1.445,-0.985,-1.071,-1.335, 0.189,-0.780, 1.160])\n",
        "# когда данных мало а признаков много\n",
        "# alpha = 0 | lambda = 0.1 -> Ridge\n",
        "ridge = LinearRegression(labelCol='consumption', elasticNetParam=0, regParam=0.1)\n",
        "ridge.fit(cars_train)\n",
        "\n",
        "# alpha = 1 | lambda = 0.1 -> Lasso\n",
        "lasso = LinearRegression(labelCol='consumption', elasticNetParam=1, regParam=0.1)\n",
        "lasso.fit(cars_train)"
      ],
      "metadata": {
        "id": "dx9p7jce2eZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensembles & Pipelines"
      ],
      "metadata": {
        "id": "WqUCd6xO57_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = StringIndexer(inputCol='type', outputCol='type_idx')\n",
        "onehot = OneHotEncoder(inputCols=['type_idx'], outputCols=['type_dummy'])\n",
        "assemble = VectorAssembler(inputCols=['mass', 'cyl', 'type_dummy'], outputCol='features')\n",
        "regression = LinearRegression(labelCol='consumption')"
      ],
      "metadata": {
        "id": "dsiesmrV6V6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=[indexer, onehot, assemble, regression])\n",
        "pipeline = pipeline.fit(cars_train)\n",
        "predictions = pipeline.transform(cars_test)\n",
        "# Конвейерный метод transform() будет вызывать метод transform() только для каждого из этапов конвейера\n",
        "pipeline.stages[3]\n",
        "print(pipeline.stages[3].intercept) # 4.1943\n",
        "print(pipeline.stages[3].coefficients) #DenseVector([0.0028,0.2705,-1.1813,-1.3696,-1.1751,-1.1553,-1.8894])"
      ],
      "metadata": {
        "id": "LKp_Mrt_Ezl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# для кросс-валидации  (?) тут круто данные перетасовать\n",
        "regression = LinearRegression(labelCol='consumption')\n",
        "evaluator = RegressionEvaluator(labelCol='consumption')\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "params = ParamGridBuilder().build()\n",
        "cv = CrossValidator(estimator=regression, estimatorParamMaps=params,\n",
        "                    evaluator=evaluator, numFolds=10, seed=13)\n",
        "# нужна модель, оценщик качества, сетка (пустая), объект CrossValidatior\n",
        "cv = cv.fit(cars_train)\n",
        "cv.avgMetrics # [0.800663722151572] RMSE\n",
        "evaluator.evaluate(cv.transform(cars_test)) # на тесте\n",
        "# обучение (бьем на фолды) - тест (один раз)"
      ],
      "metadata": {
        "id": "RCflVJ8jMPua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid-Search\n",
        "# сравниваем модель с константой и без\n",
        "regression = LinearRegression(labelCol='consumption', fitIntercept=True)\n",
        "regression = regression.fit(cars_train)\n",
        "evaluator.evaluate(regression.transform(cars_test))\n",
        "\n",
        "regression = LinearRegression(labelCol='consumption', fitIntercept=False)\n",
        "regression = regression.fit(cars_train) # без константы\n",
        "\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "params = ParamGridBuilder()\n",
        "params = params.addGrid(regression.fitIntercept, [True, False])\n",
        "params = params.build()\n",
        "print('Number of models to be tested: ', len(params)) # кол-во параметров\n",
        "\n",
        "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator)\n",
        "cv = cv.setNumFolds(10).setSeed(13).fit(cars_train) # утсановка параметров\n",
        "cv.avgMetrics # [0.800663722151, 0.907977823182]\n",
        "cv.bestModel # лучшая модель\n",
        "predictions = cv.transform(cars_test) # ведет себя как лучшая модель\n",
        "cv.bestModel.explainParam('fitIntercept')"
      ],
      "metadata": {
        "id": "Lt8W0o28PX1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#сложная сетка\n",
        "params = ParamGridBuilder().addGrid(regression.fitIntercept, [True, False]) \\\n",
        "                          .addGrid(regression.regParam, [0.001, 0.01, 0.1, 1, 10]) \\\n",
        "                          .addGrid(regression.elasticNetParam, [0, 0.25, 0.5, 0.75, 1]).build()\n",
        "print ('Number of models to be tested: ', len(params)) # 50"
      ],
      "metadata": {
        "id": "nI2y0geQgDY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ансамбль"
      ],
      "metadata": {
        "id": "1cC7JpzFgkdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# случайный лес\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "forest = RandomForestClassifier(numTrees=5)\n",
        "forest = forest.fit(cars_train)\n",
        "forest.trees # отдельные деревья\n",
        "# после трансформ получается столбец вероятности и лейблов\n",
        "+-----+-----------------------+\n",
        "|label|probability |prediction|\n",
        "+-----+-----------------------+\n",
        "| 0.0 |  [0.8,0.2] |      0.0 |\n",
        "\n",
        "forest.featureImportances # важность признаков\n",
        "# SparseVector(6, {0: 0.0205, 1: 0.2701, 2: 0.108, 3: 0.1895, 4: 0.2939, 5: 0.1181})\n"
      ],
      "metadata": {
        "id": "qvqHnIQ6gm_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# градиентный бустинг\n",
        "# дерево - добавить в ансамбль - прогноз ансамбля - сравнение предсказаний -  смотрим где неверно\n",
        "# строим другое дерево на улучшение неверных\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(maxIter=10)\n",
        "gbt = gbt.fit(cars_train) # потом все сравнивается по ROC-AUC"
      ],
      "metadata": {
        "id": "DbOzcakrwUww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}