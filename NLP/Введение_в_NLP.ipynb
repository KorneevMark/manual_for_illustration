{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Регулярные выражение"
      ],
      "metadata": {
        "id": "bHk3iUonRzpD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GPQ0_NGM8sH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb40aca-6c59-4808-f298-bd9d464de4d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Split', 'on', 'spaces.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import re\n",
        "re.match('abc', 'abcdef') #сопоставление шаблон со строкой, возврат объект соответствия\n",
        "# <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
        "\n",
        "word_regex = '\\w+' #соответствует слову\n",
        "re.match(word_regex, 'hi there!')\n",
        "# <_sre.SRE_Match object; span=(0, 2), match='hi'>\n",
        "\n",
        "re.split('\\s+', 'Split on spaces.') #['Split', 'on', 'spaces.']\n",
        "# если сплировать на основе знаков припинания то они не войдут"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so. Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
        "# разделение стороки на основе: . ? !\n",
        "sentence_endings = r\"[.?!]\"\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# поиск слов с заглавной буквы\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "# ['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
        "\n",
        "spaces = r\"\\s+\" # разеделение на основе пробела\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "digits = r\"\\d+\" # поиск всех чисел\n",
        "print(re.findall(digits, my_string))"
      ],
      "metadata": {
        "id": "ad5xzWFRi6lL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2d35a6-11bb-4ecb-81be-48988ef27f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', ' Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разница между search и match"
      ],
      "metadata": {
        "id": "a4eNKFB9Ux6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#разница между search и match\n",
        "print(re.match('cd', 'abcde')) # в середине не находит\n",
        "print(re.search('cd', 'abcde'))\n",
        "\n",
        "#пример\n",
        "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "print(match.start(), match.end()) # индексы\n",
        "\n",
        "\n",
        "pattern1 = r\"\\[.*]\"\n",
        "print(re.search(pattern1, scene_one))\n",
        "#<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
        "# In [2]: print(len('SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop]'))\n",
        "#          76\n",
        "#In [3]: len('SCENE 1: [wind] [clop clop clop]')\n",
        "#Out[3]: 32\n",
        "\n",
        "pattern2 = r\"[\\w\\s]+:\"\n",
        "print(re.match(pattern2, 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.'))\n",
        "#<re.Match object; span=(0, 7), match='ARTHUR:'>\n",
        "#'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.'\n",
        "#Помните, что вам нужно будет сопоставить любые слова или пробелы, которые предшествуют:(например, пробел внутри SOLDIER #1:)."
      ],
      "metadata": {
        "id": "yq5BlgXvpmAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c9bc7e-37ba-4b98-a1f0-e46f30715e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(2, 4), match='cd'>\n",
            "580 588\n",
            "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
            "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OR в токенизации\n",
        "import re\n",
        "match_digits_and_words = ('(\\d+|\\w+)')\n",
        "re.findall(match_digits_and_words, 'He has 11 cats.') #['He', 'has', '11', 'cats']"
      ],
      "metadata": {
        "id": "SutWTEN2tJai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47e5c8d-a087-4019-b11d-2c3e8d3cae12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He', 'has', '11', 'cats']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#range (квадратные (от и до)) и groups (круглые (только то что внутри))\n",
        "import re\n",
        "my_str = 'match lowercase spaces nums like 12, but no commas' # стоп у запятой\n",
        "re.match('[a-z0-9 ]+', my_str) #<_sre.SRE_Match object; span=(0, 42), match='match lowercase spaces nums like 12'>\n",
        "\n",
        "#пример\n",
        "regexp_tokenize(my_string, pattern2) # '(\\w+|#\\d|\\?|!)'\n",
        "#Out[3]:\n",
        "#['SOLDIER','#1','Found','them','?','In','Mercea','?','The','coconut','s','tropical','!']"
      ],
      "metadata": {
        "id": "KiV0colnyQKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95f8ab7-8dfd-420e-e8fe-91efc36a5d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация"
      ],
      "metadata": {
        "id": "qo4GeAlFRuQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "word_tokenize(\"Hi there!\")\n",
        "#sent_tokenize - разбивает документ на предложения\n",
        "#regexp_tokenize - использует регулярные выражения для токенизации строки\n",
        "#TweetTokenizer - делает аккуратные вещи, такие как распознавание хэштегов, упоминаний и когда у вас слишком много знаков препинания после предложения"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqHtBbMsRwOb",
        "outputId": "cd131ae8-5cab-4dd4-c40d-db051938fa24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi', 'there', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#новые функции (порядок изменился) ffffffffffffffffffffffffffffffffffffffffffffff\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "pattern2 = r\"([\\@\\#]\\w+)\" #можно без ()# если r\"(\\@|\\#)\\w+\" то ['@', '#', '#']\n",
        "\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags) # ['@datacamp', '#nlp', '#python']\n",
        "\n",
        "# ['This is the best #nlp exercise ive found online! #python',\n",
        "# '#NLP is super fun! <3 #learning',\n",
        "# 'Thanks @datacamp :) #nlp #python']\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens) #[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'],\n",
        " #['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n",
        "\n",
        "capital_words = r\"[A-ZÜ]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words)) #['Wann', 'Pizza', 'Und', 'Über']"
      ],
      "metadata": {
        "id": "U6fIXD3i8LVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(\"This is a pretty cool tool!\")\n",
        "word_lengths = [len(w) for w in words]\n",
        "plt.hist(word_lengths)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "9dZhJNpc3DWb",
        "outputId": "feb86849-a2ff-4965-e7b8-e8bc0bb63b50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize # bag-of-words\n",
        "from collections import Counter\n",
        "counter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))\n",
        "#Counter({'.': 3,'The': 3, 'box': 3, 'cat': 3})\n",
        "counter.most_common(2) #[('The', 3), ('box', 3)]"
      ],
      "metadata": {
        "id": "8LVUBYoT6bf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e75a7fb9-8305-4388-a131-674ff06de606"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 3), ('cat', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#препроцессинг (множественное в единственное, прописные в строчные, стоп слова)\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "text = \"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"\n",
        "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()] #isalpha True если это в строке только буквы\n",
        "no_stops = [t for t in tokens if t not in stopwords.words('english')] #удаление стоп-слов которые встрены\n",
        "Counter(no_stops).most_common(2) #[('cat', 3), ('box', 3)]"
      ],
      "metadata": {
        "id": "z14KZKcm_ZKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e8840c-a280-4f8e-c7e8-2a4ed3de7a1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 3), ('box', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# пример с лимматизацией\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "bow = Counter(lemmatized)\n",
        "print(bow.most_common(10))"
      ],
      "metadata": {
        "id": "5-g8qHuYUT7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction to gensim"
      ],
      "metadata": {
        "id": "ytlI9Pwpk5gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "my_documents = ['The movie was about a spaceship and aliens.', 'I really liked the movie!',\n",
        "                'Awesome action scenes, but boring characters.', 'The movie was awful! I hate alien films.',\n",
        "                'Space is cool! I liked the movie.', 'More space films, please!']\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "dictionary.token2id # {'!': 11,',': 17,'.': 7, 'a': 2...}\n",
        "\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "#[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
        "#[(0, 1), (1, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n",
        "# каждый список это предложение, (номер слова и частота)\n",
        "# создали корпус Gensim - продвинутую и многофункциональную модель набора слов\n",
        "corpus"
      ],
      "metadata": {
        "id": "N7fEuDB0Br4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23727192-a504-452c-8bab-c46cd088ce89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
              " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
              " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
              " [(0, 1),\n",
              "  (5, 1),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 1),\n",
              "  (10, 1),\n",
              "  (20, 1),\n",
              "  (21, 1),\n",
              "  (22, 1),\n",
              "  (23, 1)],\n",
              " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
              " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример"
      ],
      "metadata": {
        "id": "w-schychs91c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "my_documents = ['The movie was about a spaceship and aliens.', 'I really liked the movie!',\n",
        "                'Awesome action scenes, but boring characters.', 'The movie was awful! I hate alien films.',\n",
        "                'Space is cool! I liked the movie.', 'More space films, please!']\n",
        "articles = [word_tokenize(doc.lower()) for doc in my_documents]\n",
        "dictionary = Dictionary(articles)\n",
        "computer_id = dictionary.token2id.get(\"space\") # индекс слова\n",
        "print(dictionary.get(computer_id)) # по индексу слово"
      ],
      "metadata": {
        "id": "iG5l-kjDdy_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e266df8a-bac9-4139-f8c2-840bb770f76a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles"
      ],
      "metadata": {
        "id": "rywdY02mSXkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191acb1f-bc13-4751-e4e6-d86e2683aa8a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the', 'movie', 'was', 'about', 'a', 'spaceship', 'and', 'aliens', '.'],\n",
              " ['i', 'really', 'liked', 'the', 'movie', '!'],\n",
              " ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'],\n",
              " ['the', 'movie', 'was', 'awful', '!', 'i', 'hate', 'alien', 'films', '.'],\n",
              " ['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.'],\n",
              " ['more', 'space', 'films', ',', 'please', '!']]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "computer_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkyKHS1IsqwU",
        "outputId": "c5a44eca-5956-482c-fd0d-404848ba8177"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = corpus[4] #ааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True) # сортировка по частоте\n",
        "\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count) # вывести 5 слов\n",
        "\n",
        "total_word_count = defaultdict(int) # посчитать кол-во каждого слова везде\n",
        "\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "\n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
        "#In [14]: total_word_count.items()\n",
        "#Out[14]: dict_items([(0, 1006), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1)\n",
        "\n",
        "#In [14]: total_word_count\n",
        "#Out[14]: defaultdict(int, {0: 1006, 1: 1, 2: 1, 3: 2, 4: 1, 5: 1, 6: 1,\n",
        "                            7: 1, 8: 177, 9: 2, 10: 59, 11: 5...\n",
        "for word_id, word_count in sorted_word_count[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "# engineering 91\n",
        "#    '' 85\n",
        "#    reverse 73"
      ],
      "metadata": {
        "id": "kpz1oj3Vs2Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tf-idf с gensim"
      ],
      "metadata": {
        "id": "Sa8fa_SCIbYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "tfidf = TfidfModel(corpus) # используем Gensim корпус\n",
        "tfidf[corpus[1]] # выбрать документ\n",
        "# [(0, 0.1746),(1, 0.1746), (9, 0.2985), (10, 0.7716),...]"
      ],
      "metadata": {
        "id": "P3MJebisIcY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfModel(corpus)\n",
        "tfidf_weights = tfidf[doc]\n",
        "print(tfidf_weights[:5])\n",
        "\n",
        "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)"
      ],
      "metadata": {
        "id": "0GKHgPTEqjBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Распознавание именованных сущностей (Named Entity Recognition)"
      ],
      "metadata": {
        "id": "HmZHcP8wNCso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#отвечаем на вопрос кто? что? когда? и где?\n",
        "import nltk # Стэнфордская библиотека CoreNLP\n",
        "#nltk.download('all')\n",
        "sentence = '''In New York, I like to ride the Metro to visit MOMA and some restaurants rated well by Ruth Reichl.'''\n",
        "tokenized_sent = nltk.word_tokenize(sentence)\n",
        "tagged_sent = nltk.pos_tag(tokenized_sent) #распознаем части речи\n",
        "tagged_sent[:3] #[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n",
        "# NNP - собственного существительного единственного числа\n",
        "print(nltk.ne_chunk(tagged_sent)) # вернет предложение в виде дерева\n",
        "# (S\n",
        "#In/IN\n",
        "# (GPE New/NNP York/NNP)\n",
        "#  ,/,\n",
        "#  I/PRP\n",
        "#  like/VBP\n",
        "#  to/TO\n",
        "#  ride/VB\n",
        "#  the/DT\n",
        "#  (ORGANIZATION Metro/NNP)"
      ],
      "metadata": {
        "id": "zEKqW2Q9NHFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e00803-11b7-42c8-d982-af13606027ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  In/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  like/VBP\n",
            "  to/TO\n",
            "  ride/VB\n",
            "  the/DT\n",
            "  (ORGANIZATION Metro/NNP)\n",
            "  to/TO\n",
            "  visit/VB\n",
            "  (ORGANIZATION MOMA/NNP)\n",
            "  and/CC\n",
            "  some/DT\n",
            "  restaurants/NNS\n",
            "  rated/VBN\n",
            "  well/RB\n",
            "  by/IN\n",
            "  (PERSON Ruth/NNP Reichl/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "sentences = nltk.sent_tokenize(article) # разделение предложений на ['...', '...']\n",
        "token_sentences = [nltk.word_tokenize(sent) for sent in sentences] # [['...'], ['...']]\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
        "\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
        "# (S\n",
        "  Yet/RB\n",
        "  there/EX\n",
        "  ’/NNP\n",
        "  task/NN\n",
        " ...\n",
        "  ./.)\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)\n",
        "#является ли это фрагментом именованной сущности, проверив, есть ли у него метка атрибута и является ли фрагмент.label() равно \"NE\"\n",
        "(NE CEO/NNP)\n",
        "(NE Yahoo/NNP)\n",
        "(NE Marissa/NNP Mayer/NNP)"
      ],
      "metadata": {
        "id": "XliRyOiKFh6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на вход  Tree('S', [('“', 'NN'), ('One', 'CD'), ('has', 'VBZ'), ('benefits', 'NNS'), ('and', 'CC'), ('risks', 'NNS'), ('.', '.'), ('”', 'NN')])]\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "for sent in chunked_sentences: # выход (S “/NN  One/CD has/VBZ to/TO ./. ”/NN)\n",
        "    for chunk in sent: # ('ensure', 'VB') ('the', 'DT') ('best', 'JJS') ('exploitation', 'NN')\n",
        "        if hasattr(chunk, 'label'): # проверяет существование атрибута с именем name в объекте object\n",
        "            ner_categories[chunk.label()] += 1\n",
        "#defaultdict(<class 'int'>, {'ORGANIZATION': 33, 'GPE': 115, 'PERSON': 70, 'LOCATION': 1, 'FACILITY': 1})\n",
        "        print(chunk)\n",
        "\n",
        "labels = list(ner_categories.keys())\n",
        "values = [ner_categories.get(v) for v in labels] # [33, 115, 70, 1, 1]\n",
        "\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "50gadOvZLSpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction to\n",
        "SpaCy"
      ],
      "metadata": {
        "id": "8Xkll8MmSEl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy # это библиотека NLP, похожая на Gensim\n",
        "nlp = spacy.load('en_core_web_sm') #загрузить все соответствующие предварительно обученные векторы слов\n",
        "nlp.entity #функционирует аналогично нашему словарю и корпусу Gensim\n",
        "\n",
        "doc = nlp(\"\"\"Berlin is the capital of Germany; and the residence of Chancellor Angela Merkel.\"\"\")\n",
        "doc.ents #(Berlin, Germany, Angela Merkel) именованные сущности\n",
        "\n",
        "print(doc.ents[0], doc.ents[0].label_) #Berlin GPE\n",
        "\n",
        "#пример\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'matcher'])\n",
        "doc = nlp(article)\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_, ent.text) #ORG Apple  PERSON Travis Kalanick of Uber   PERSON Tim Cook\n",
        "# если print(ent) Apple Travis Kalanick of Uber Tim Cook Apple"
      ],
      "metadata": {
        "id": "0CDG3f2cSFru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polyglot"
      ],
      "metadata": {
        "id": "jozzbWrzbZb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from polyglot.text import Text # именнованные сущности других языков\n",
        "text = \"\"\"El presidente de la Generalitat de Cataluña, Carles Puigdemont, ha afirmado hoy a la alcaldesa de Madrid, Manuela Carmena, que en su etapa de alcalde de Girona (de julio de 2011 a enero de 2016) hizo una gran promoción de Madrid.\"\"\"\n",
        "ptext = Text(text)\n",
        "ptext.entities\n",
        "# [I-ORG(['Generalitat','de']),\n",
        "# I-LOC(['Generalitat','de','Cataluña']),\n",
        "# I-PER(['Carles','Puigdemont']),\n",
        "# I-LOC(['Madrid']),"
      ],
      "metadata": {
        "id": "02VsD9V-bcGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Классификация фейковых новостей с помощью контролируемого обучения с НЛП"
      ],
      "metadata": {
        "id": "MaptSWLwcp8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "df =...\n",
        "y = df['Sci-Fi'] # REAL FAKE REAL REAL\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['plot'], y, test_size=0.33, random_state=53)\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_train = count_vectorizer.fit_transform(X_train.values)\n",
        "count_test = count_vectorizer.transform(X_test.values)\n",
        "\n",
        "print(count_vectorizer.get_feature_names()[:10])"
      ],
      "metadata": {
        "id": "SEQvCalBcqvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(tfidf_vectorizer.get_feature_names()[:10])\n",
        "\n",
        "print(tfidf_train.A[:5])\n",
        "#['00', '000', '001', '008s', '00am', '00pm', '01', '01am', '02', '024']\n",
        "#[[0.         0.01928563 0.         ... 0.         0.         0.        ]\n",
        "# [0.         0.         0.         ... 0.         0.         0.        ]\n",
        "\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "print(count_df.head())\n",
        "print(tfidf_df.head())\n",
        "\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "print(difference)\n",
        "print(count_df.equals(tfidf_df))"
      ],
      "metadata": {
        "id": "RXCq0DeAuE4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes classifier"
      ],
      "metadata": {
        "id": "F86oioVNkIVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "pred = nb_classifier.predict(count_test)\n",
        "metrics.accuracy_score(y_test, pred)\n",
        "\n",
        "metrics.confusion_matrix(y_test, pred, labels=[0,1])\n",
        "\n",
        "#пример (In [1]:type(tfidf_train) Out[1]: scipy.sparse.csr.csr_matrix)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "pred = nb_classifier.predict(count_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)\n",
        "\n",
        "# что?\n",
        "class_labels = nb_classifier.classes_\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
        "print(class_labels[0], feat_with_weights[:20])\n",
        "print(class_labels[1], feat_with_weights[-20:])"
      ],
      "metadata": {
        "id": "fpGBF8amkHph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}